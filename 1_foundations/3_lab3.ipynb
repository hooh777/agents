{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🌟 Lab 3: Personal Website Chatbot with Quality Control\n",
    "\n",
    "## 🎯 What You'll Build\n",
    "Today we're building a smart chatbot for your personal website that:\n",
    "- **Represents you professionally** using your LinkedIn profile and summary\n",
    "- **Has quality control** - automatically evaluates responses and retries if needed\n",
    "- **Works with your own data** - personalizes responses using your background\n",
    "\n",
    "## 📁 Setup Required\n",
    "1. Replace `me/linkedin.pdf` with your own LinkedIn profile PDF\n",
    "2. Update `me/summary.txt` with your personal summary\n",
    "3. Change the `name` variable to your name in the code\n",
    "\n",
    "## 🛤️ Two Complete Paths Available\n",
    "\n",
    "### Path A: Original (OpenAI + Gemini)\n",
    "- Uses OpenAI GPT-4o-mini for chat\n",
    "- Uses Gemini for evaluation\n",
    "- Follow sections 1-4 in order\n",
    "\n",
    "### Path B: Complete Qwen Version (Recommended)\n",
    "- Uses Qwen-Plus for chat  \n",
    "- Uses Qwen-Max for evaluation\n",
    "- Fully self-contained, no geographic restrictions\n",
    "- **Skip to Section 5** if you choose this path\n",
    "\n",
    "**Choose one path and follow it completely!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/tools.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#00bfff;\">Looking up packages</h2>\n",
    "            <span style=\"color:#00bfff;\">In this lab, we're going to use the wonderful Gradio package for building quick UIs, \n",
    "            and we're also going to use the popular PyPDF PDF reader. You can get guides to these packages by asking \n",
    "            ChatGPT or Claude, and you find all open-source packages on the repository <a href=\"https://pypi.org\">https://pypi.org</a>.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📚 Section 1: Core Dependencies & Data Loading\n",
    "\n",
    "This section loads the essential packages and your personal data that both paths will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you don't know what any of these packages do - you can always ask ChatGPT for a guide!\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from pypdf import PdfReader\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Environment variables loaded\n",
      "📋 Available APIs will depend on your chosen path\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables\n",
    "load_dotenv(override=True)\n",
    "\n",
    "print(\"✅ Environment variables loaded\")\n",
    "print(\"📋 Available APIs will depend on your chosen path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LinkedIn profile loaded (8256 characters)\n",
      "📄 Replace me/linkedin.pdf with your own profile!\n"
     ]
    }
   ],
   "source": [
    "# Load your LinkedIn profile PDF\n",
    "reader = PdfReader(\"me/linkedin.pdf\")\n",
    "linkedin = \"\"\n",
    "for page in reader.pages:\n",
    "    text = page.extract_text()\n",
    "    if text:\n",
    "        linkedin += text\n",
    "\n",
    "print(f\"✅ LinkedIn profile loaded ({len(linkedin)} characters)\")\n",
    "print(\"📄 Replace me/linkedin.pdf with your own profile!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   \n",
      "Contact\n",
      "ed.donner@gmail.com\n",
      "www.linkedin.com/in/eddonner\n",
      "(LinkedIn)\n",
      "edwarddonner.com (Personal)\n",
      "Top Skills\n",
      "CTO\n",
      "Large Language Models (LLM)\n",
      "PyTorch\n",
      "Patents\n",
      "Apparatus for determining role\n",
      "fitness while eliminating unwanted\n",
      "bias\n",
      "Ed Donner\n",
      "Co-Founder & CTO at Nebula.io, repeat Co-Founder of AI startups,\n",
      "speaker & advisor on Gen AI and LLM Engineering\n",
      "New York, New York, United States\n",
      "Summary\n",
      "I’m a technology leader and entrepreneur. I'm applying AI to a field\n",
      "where it can make a massive impact: helping people discover their\n",
      "potential and pursue their reason for being. But at my core, I’m a\n",
      "software engineer and a scientist. I learned how to code aged 8 and\n",
      "still spend weekends experimenting with Large Language Models\n",
      "and writing code (rather badly). If you’d like to join us to show me\n",
      "how it’s done.. message me!\n",
      "As a work-hobby, I absolutely love giving talks about Gen AI and\n",
      "LLMs. I'm the author of a best-selling, top-rated Udemy course\n",
      "on LLM Engineering, and I speak at O'Reilly Live Events and\n",
      "ODSC workshops. It brings me great joy to help others unlock the\n",
      "astonishing power of LLMs.\n",
      "I spent most of my career at JPMorgan building software for financial\n",
      "markets. I worked in London, Tokyo and New York. I became an MD\n",
      "running a global organization of 300. Then I left to start my own AI\n",
      "business, untapt, to solve the problem that had plagued me at JPM -\n",
      "why is so hard to hire engineers?\n",
      "At untapt we worked with GQR, one of the world's fastest growing\n",
      "recruitment firms. We collaborated on a patented invention in AI\n",
      "and talent. Our skills were perfectly complementary - AI leaders vs\n",
      "recruitment leaders - so much so, that we decided to join forces. In\n",
      "2020, untapt was acquired by GQR’s parent company and Nebula\n",
      "was born.\n",
      "I’m now Co-Founder and CTO for Nebula, responsible for software\n",
      "engineering and data science.  Our stack is Python/Flask, React,\n",
      "Mongo, ElasticSearch, with Kubernetes on GCP. Our 'secret sauce'\n",
      "is our use of Gen AI and proprietary LLMs. If any of this sounds\n",
      "interesting - we should talk!\n",
      "  Page 1 of 5   \n",
      "Experience\n",
      "Nebula.io\n",
      "Co-Founder & CTO\n",
      "June 2021 - Present (3 years 10 months)\n",
      "New York, New York, United States\n",
      "I’m the co-founder and CTO of Nebula.io. We help recruiters source,\n",
      "understand, engage and manage talent, using Generative AI / proprietary\n",
      "LLMs. Our patented model matches people with roles with greater accuracy\n",
      "and speed than previously imaginable — no keywords required.\n",
      "Our long term goal is to help people discover their potential and pursue their\n",
      "reason for being, motivated by a concept called Ikigai. We help people find\n",
      "roles where they will be most fulfilled and successful; as a result, we will raise\n",
      "the level of human prosperity. It sounds grandiose, but since 77% of people\n",
      "don’t consider themselves inspired or engaged at work, it’s completely within\n",
      "our reach.\n",
      "Simplified.Travel\n",
      "AI Advisor\n",
      "February 2025 - Present (2 months)\n",
      "Simplified Travel is empowering destinations to deliver unforgettable, data-\n",
      "driven journeys at scale.\n",
      "I'm giving AI advice to enable highly personalized itinerary solutions for DMOs,\n",
      "hotels and tourism organizations, enhancing traveler experiences.\n",
      "GQR Global Markets\n",
      "Chief Technology Officer\n",
      "January 2020 - Present (5 years 3 months)\n",
      "New York, New York, United States\n",
      "As CTO of parent company Wynden Stark, I'm also responsible for innovation\n",
      "initiatives at GQR.\n",
      "Wynden Stark\n",
      "Chief Technology Officer\n",
      "January 2020 - Present (5 years 3 months)\n",
      "New York, New York, United States\n",
      "With the acquisition of untapt, I transitioned to Chief Technology Officer for the\n",
      "Wynden Stark Group, responsible for Data Science and Engineering.\n",
      "  Page 2 of 5   \n",
      "untapt\n",
      "6 years 4 months\n",
      "Founder, CTO\n",
      "May 2019 - January 2020 (9 months)\n",
      "Greater New York City Area\n",
      "I founded untapt in October 2013; emerged from stealth in 2014 and went\n",
      "into production with first product in 2015. In May 2019, I handed over CEO\n",
      "responsibilities to Gareth Moody, previously the Chief Revenue Officer, shifting\n",
      "my focus to the technology and product.\n",
      "Our core invention is an Artificial Neural Network that uses Deep Learning /\n",
      "NLP to understand the fit between candidates and roles.\n",
      "Our SaaS products are used in the Recruitment Industry to connect people\n",
      "with jobs in a highly scalable way. Our products are also used by Corporations\n",
      "for internal and external hiring at high volume. We have strong SaaS metrics\n",
      "and trends, and a growing number of bellwether clients.\n",
      "Our Deep Learning / NLP models are developed in Python using Google\n",
      "TensorFlow. Our tech stack is React / Redux and Angular HTML5 front-end\n",
      "with Python / Flask back-end and MongoDB database. We are deployed on\n",
      "the Google Cloud Platform using Kubernetes container orchestration.\n",
      "Interview at NASDAQ: https://www.pscp.tv/w/1mnxeoNrEvZGX\n",
      "Founder, CEO\n",
      "October 2013 - May 2019 (5 years 8 months)\n",
      "Greater New York City Area\n",
      "I founded untapt in October 2013; emerged from stealth in 2014 and went into\n",
      "production with first product in 2015.\n",
      "Our core invention is an Artificial Neural Network that uses Deep Learning /\n",
      "NLP to understand the fit between candidates and roles.\n",
      "Our SaaS products are used in the Recruitment Industry to connect people\n",
      "with jobs in a highly scalable way. Our products are also used by Corporations\n",
      "for internal and external hiring at high volume. We have strong SaaS metrics\n",
      "and trends, and a growing number of bellwether clients.\n",
      "  Page 3 of 5   \n",
      "Our Deep Learning / NLP models are developed in Python using Google\n",
      "TensorFlow. Our tech stack is React / Redux and Angular HTML5 front-end\n",
      "with Python / Flask back-end and MongoDB database. We are deployed on\n",
      "the Google Cloud Platform using Kubernetes container orchestration.\n",
      "-- Graduate of FinTech Innovation Lab\n",
      "-- American Banker Top 20 Company To Watch\n",
      "-- Voted AWS startup most likely to grow exponentially\n",
      "-- Forbes contributor\n",
      "More at https://www.untapt.com\n",
      "Interview at NASDAQ: https://www.pscp.tv/w/1mnxeoNrEvZGX\n",
      "In Fast Company: https://www.fastcompany.com/3067339/how-artificial-\n",
      "intelligence-is-changing-the-way-companies-hire\n",
      "JPMorgan Chase\n",
      "11 years 6 months\n",
      "Managing Director\n",
      "May 2011 - March 2013 (1 year 11 months)\n",
      "Head of Technology for the Credit Portfolio Group and Hedge Fund Credit in\n",
      "the JPMorgan Investment Bank.\n",
      "Led a team of 300 Java and Python software developers across NY, Houston,\n",
      "London, Glasgow and India. Responsible for counterparty exposure, CVA\n",
      "and risk management platforms, including simulation engines in Python that\n",
      "calculate counterparty credit risk for the firm's Derivatives portfolio.\n",
      "Managed the electronic trading limits initiative, and the Credit Stress program\n",
      "which calculates risk information under stressed conditions. Jointly responsible\n",
      "for Market Data and batch infrastructure across Risk.\n",
      "Executive Director\n",
      "January 2007 - May 2011 (4 years 5 months)\n",
      "From Jan 2008:\n",
      "Chief Business Technologist for the Credit Portfolio Group and Hedge Fund\n",
      "Credit in the JPMorgan Investment Bank, building Java and Python solutions\n",
      "and managing a team of full stack developers.\n",
      "2007:\n",
      "  Page 4 of 5   \n",
      "Responsible for Credit Risk Limits Monitoring infrastructure for Derivatives and\n",
      "Cash Securities, developed in Java / Javascript / HTML.\n",
      "VP\n",
      "July 2004 - December 2006 (2 years 6 months)\n",
      "Managed Collateral, Netting and Legal documentation technology across\n",
      "Derivatives, Securities and Traditional Credit Products, including Java, Oracle,\n",
      "SQL based platforms\n",
      "VP\n",
      "October 2001 - June 2004 (2 years 9 months)\n",
      "Full stack developer, then manager for Java cross-product risk management\n",
      "system in Credit Markets Technology\n",
      "Cygnifi\n",
      "Project Leader\n",
      "January 2000 - September 2001 (1 year 9 months)\n",
      "Full stack developer and engineering lead, developing Java and Javascript\n",
      "platform to risk manage Interest Rate Derivatives at this FInTech startup and\n",
      "JPMorgan spin-off.\n",
      "JPMorgan\n",
      "Associate\n",
      "July 1997 - December 1999 (2 years 6 months)\n",
      "Full stack developer for Exotic and Flow Interest Rate Derivatives risk\n",
      "management system in London, New York and Tokyo\n",
      "IBM\n",
      "Software Developer\n",
      "August 1995 - June 1997 (1 year 11 months)\n",
      "Java and Smalltalk developer with IBM Global Services; taught IBM classes on\n",
      "Smalltalk and Object Technology in the UK and around Europe\n",
      "Education\n",
      "University of Oxford\n",
      "Physics  · (1992 - 1995)\n",
      "  Page 5 of 5\n"
     ]
    }
   ],
   "source": [
    "print(linkedin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Personal summary loaded (387 characters)\n",
      "📝 Update me/summary.txt with your own background!\n"
     ]
    }
   ],
   "source": [
    "# Load your personal summary\n",
    "with open(\"me/summary.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    summary = f.read()\n",
    "    \n",
    "print(f\"✅ Personal summary loaded ({len(summary)} characters)\")\n",
    "print(\"📝 Update me/summary.txt with your own background!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Chatbot will represent: Ed Donner\n",
      "⚠️  Remember to change this to YOUR name!\n"
     ]
    }
   ],
   "source": [
    "# 🔧 PERSONALIZE THIS: Change to your name!\n",
    "name = \"Ed Donner\"\n",
    "\n",
    "print(f\"✅ Chatbot will represent: {name}\")\n",
    "print(\"⚠️  Remember to change this to YOUR name!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ System prompt created\n",
      "📏 Prompt length: 9305 characters\n",
      "🎭 Your chatbot now has personality and context!\n"
     ]
    }
   ],
   "source": [
    "# Create the system prompt that defines the chatbot's personality and role\n",
    "system_prompt = f\"You are acting as {name}. You are answering questions on {name}'s website, \\\n",
    "particularly questions related to {name}'s career, background, skills and experience. \\\n",
    "Your responsibility is to represent {name} for interactions on the website as faithfully as possible. \\\n",
    "You are given a summary of {name}'s background and LinkedIn profile which you can use to answer questions. \\\n",
    "Be professional and engaging, as if talking to a potential client or future employer who came across the website. \\\n",
    "If you don't know the answer, say so.\"\n",
    "\n",
    "system_prompt += f\"\\n\\n## Summary:\\n{summary}\\n\\n## LinkedIn Profile:\\n{linkedin}\\n\\n\"\n",
    "system_prompt += f\"With this context, please chat with the user, always staying in character as {name}.\"\n",
    "\n",
    "print(\"✅ System prompt created\")\n",
    "print(f\"📏 Prompt length: {len(system_prompt)} characters\")\n",
    "print(\"🎭 Your chatbot now has personality and context!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🛤️ PATH SELECTION: Choose Your Route\n",
    "\n",
    "**All core data loaded!** ✅ Now choose which path to follow:\n",
    "\n",
    "## 📍 Option A: Continue with OpenAI + Gemini Path\n",
    "**Next:** Run Section 2 cells below ⬇️\n",
    "- Uses OpenAI GPT-4o-mini for chat\n",
    "- Uses Gemini for evaluation\n",
    "- Traditional approach with mixed APIs\n",
    "\n",
    "## 📍 Option B: Skip to Complete Qwen Version  \n",
    "**Next:** Jump to **Section 5** (scroll down) ⬇️⬇️⬇️\n",
    "- Uses Qwen-Plus for chat\n",
    "- Uses Qwen-Max for evaluation  \n",
    "- Self-contained, reliable access\n",
    "\n",
    "**⚠️ Important:** Pick ONE path and follow it completely - don't mix them!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📚 Section 2: OpenAI + Gemini Path Setup\n",
    "\n",
    "**Only run this section if you chose Path A!**\n",
    "\n",
    "Setting up the traditional approach with OpenAI for chat and Gemini for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI client setup\n",
    "openai = OpenAI()\n",
    "\n",
    "# Basic chat function for OpenAI\n",
    "def chat_openai(message, history):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    response = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=messages)\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "print(\"✅ OpenAI client configured\")\n",
    "print(\"🤖 Basic chat function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📚 Section 3: Test Basic Chat (OpenAI Path)\n",
    "\n",
    "Let's test the basic chatbot before adding quality control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the basic OpenAI chatbot (without quality control)\n",
    "print(\"🚀 Launching basic OpenAI chatbot for testing...\")\n",
    "print(\"🧪 This is just a test - we'll add quality control next\")\n",
    "\n",
    "gr.ChatInterface(chat_openai, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📚 Section 4: Add Quality Control (OpenAI Path)\n",
    "\n",
    "Now let's add the quality control system:\n",
    "1. **Evaluator** - Uses Gemini to check response quality\n",
    "2. **Rerun capability** - Automatically retries if response fails evaluation\n",
    "3. **Complete workflow** - Chat with automatic quality assurance\n",
    "\n",
    "## 🎯 What's About to Happen:\n",
    "1. Set up Gemini as evaluator\n",
    "2. Create evaluation logic\n",
    "3. Add rerun functionality  \n",
    "4. Launch the complete chatbot with quality control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Qwen clients ready!\n",
      "💬 Chat model: qwen-plus\n",
      "🔍 Evaluator model: qwen-max\n"
     ]
    }
   ],
   "source": [
    "# Set up Gemini as the evaluator for the OpenAI path\n",
    "import os\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# Gemini client for evaluation\n",
    "gemini = OpenAI(\n",
    "    api_key=os.getenv(\"GEMINI_API_KEY\"),\n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")\n",
    "\n",
    "# Evaluation model\n",
    "class Evaluation(BaseModel):\n",
    "    is_acceptable: bool\n",
    "    feedback: str\n",
    "\n",
    "print(\"✅ Gemini evaluator client ready\")\n",
    "print(\"\udd0d Will use Gemini to evaluate OpenAI responses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Checking Qwen Version Dependencies ===\n",
      "✅ name: Available\n",
      "✅ summary: Available\n",
      "✅ linkedin: Available\n",
      "✅ system_prompt: Available\n",
      "✅ DASHSCOPE_API_KEY: Available (sk-0958b...)\n",
      "\n",
      "🎉 ALL DEPENDENCIES READY! Qwen version can proceed.\n"
     ]
    }
   ],
   "source": [
    "# Create evaluator system prompt\n",
    "evaluator_system_prompt = f\"You are an evaluator that decides whether a response to a question is acceptable. \\\n",
    "You are provided with a conversation between a User and an Agent. Your task is to decide whether the Agent's latest response is acceptable quality. \\\n",
    "The Agent is playing the role of {name} and is representing {name} on their website. \\\n",
    "The Agent has been instructed to be professional and engaging, as if talking to a potential client or future employer who came across the website. \\\n",
    "The Agent has been provided with context on {name} in the form of their summary and LinkedIn details. Here's the information:\"\n",
    "\n",
    "evaluator_system_prompt += f\"\\n\\n## Summary:\\n{summary}\\n\\n## LinkedIn Profile:\\n{linkedin}\\n\\n\"\n",
    "evaluator_system_prompt += f\"With this context, please evaluate the latest response, replying with whether the response is acceptable and your feedback.\"\n",
    "\n",
    "def evaluator_user_prompt(reply, message, history):\n",
    "    user_prompt = f\"Here's the conversation between the User and the Agent: \\n\\n{history}\\n\\n\"\n",
    "    user_prompt += f\"Here's the latest message from the User: \\n\\n{message}\\n\\n\"\n",
    "    user_prompt += f\"Here's the latest response from the Agent: \\n\\n{reply}\\n\\n\"\n",
    "    user_prompt += \"Please evaluate the response, replying with whether it is acceptable and your feedback.\"\n",
    "    return user_prompt\n",
    "\n",
    "print(\"✅ Evaluator prompt system ready\")\n",
    "print(\"\udfaf Gemini will judge response quality\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LinkedIn PDF loaded successfully\n",
      "✅ Summary loaded successfully\n",
      "✅ Name set to: Ed Donner\n",
      "✅ System prompt created\n",
      "\n",
      "🎉 All Qwen dependencies now loaded! You can proceed with the Qwen cells below.\n"
     ]
    }
   ],
   "source": [
    "# Gemini evaluation function\n",
    "def evaluate_gemini(reply, message, history) -> Evaluation:\n",
    "    messages = [{\"role\": \"system\", \"content\": evaluator_system_prompt}] + [{\"role\": \"user\", \"content\": evaluator_user_prompt(reply, message, history)}]\n",
    "    \n",
    "    try:\n",
    "        response = gemini.chat.completions.create(\n",
    "            model=\"gemini-1.5-flash\",\n",
    "            messages=messages,\n",
    "            response_format={\"type\": \"json_object\"}\n",
    "        )\n",
    "        \n",
    "        import json\n",
    "        result = json.loads(response.choices[0].message.content)\n",
    "        return Evaluation(is_acceptable=result[\"is_acceptable\"], feedback=result[\"feedback\"])\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  Evaluation error: {e}\")\n",
    "        # Safe fallback\n",
    "        return Evaluation(is_acceptable=True, feedback=f\"Evaluation failed: {str(e)}\")\n",
    "\n",
    "print(\"✅ Gemini evaluation function ready\")\n",
    "print(\"🔍 Can now evaluate OpenAI responses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testing Qwen API Connection ===\n",
      "✅ Qwen API connection successful!\n",
      "Response: Hello, Qwen API works!\n",
      "✅ Qwen API connection successful!\n",
      "Response: Hello, Qwen API works!\n"
     ]
    }
   ],
   "source": [
    "# OpenAI rerun function when evaluation fails\n",
    "def rerun_openai(reply, message, history, feedback):\n",
    "    updated_system_prompt = system_prompt + \"\\n\\n## Previous answer rejected\\nYou just tried to reply, but the quality control rejected your reply\\n\"\n",
    "    updated_system_prompt += f\"## Your attempted answer:\\n{reply}\\n\\n\"\n",
    "    updated_system_prompt += f\"## Reason for rejection:\\n{feedback}\\n\\n\"\n",
    "    messages = [{\"role\": \"system\", \"content\": updated_system_prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    response = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=messages)\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "print(\"✅ OpenAI rerun function ready\")\n",
    "print(\"🔄 Can retry failed responses with feedback\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete OpenAI chat function with Gemini quality control\n",
    "def chat_openai_with_qc(message, history):\n",
    "    # Clean up history for compatibility\n",
    "    history = [{\"role\": h[\"role\"], \"content\": h[\"content\"]} for h in history]\n",
    "    \n",
    "    # Special behavior for patent questions (demo of context modification)\n",
    "    if \"patent\" in message.lower():\n",
    "        system = system_prompt + \"\\n\\nEverything in your reply needs to be in pig latin - \\\n",
    "              it is mandatory that you respond only and entirely in pig latin\"\n",
    "    else:\n",
    "        system = system_prompt\n",
    "    \n",
    "    # Get initial response from OpenAI\n",
    "    messages = [{\"role\": \"system\", \"content\": system}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    response = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=messages)\n",
    "    reply = response.choices[0].message.content\n",
    "\n",
    "    # Evaluate response with Gemini\n",
    "    evaluation = evaluate_gemini(reply, message, history)\n",
    "    \n",
    "    if evaluation.is_acceptable:\n",
    "        print(\"✅ Passed Gemini evaluation - returning reply\")\n",
    "        return reply\n",
    "    else:\n",
    "        print(\"❌ Failed Gemini evaluation - retrying with OpenAI\")\n",
    "        print(f\"🔍 Feedback: {evaluation.feedback}\")\n",
    "        reply = rerun_openai(reply, message, history, evaluation.feedback)       \n",
    "        print(\"🔄 Retry completed\")\n",
    "        return reply\n",
    "\n",
    "print(\"✅ Complete OpenAI + Gemini workflow ready!\")\n",
    "print(\"🤖 OpenAI chat + 🔍 Gemini evaluation + 🔄 Auto-retry\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the quality control system\n",
    "print(\"🧪 Testing OpenAI + Gemini quality control system...\")\n",
    "\n",
    "# Test with patent question (should trigger pig latin response)\n",
    "messages = [{\"role\": \"system\", \"content\": system_prompt}] + [{\"role\": \"user\", \"content\": \"do you hold a patent?\"}]\n",
    "response = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=messages)\n",
    "reply = response.choices[0].message.content\n",
    "\n",
    "print(\"📝 OpenAI Reply:\")\n",
    "print(reply)\n",
    "print()\n",
    "\n",
    "# Evaluate the reply\n",
    "evaluation = evaluate_gemini(reply, \"do you hold a patent?\", messages[:1])\n",
    "print(\"🔍 Gemini Evaluation:\")\n",
    "print(f\"Acceptable: {evaluation.is_acceptable}\")\n",
    "print(f\"Feedback: {evaluation.feedback}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🚀 Launch the complete OpenAI + Gemini website chatbot!\n",
    "print(\"🚀 Launching OpenAI + Gemini personal website chatbot...\")\n",
    "print(\"\udcac Main chat: OpenAI GPT-4o-mini\")  \n",
    "print(\"🔍 Quality control: Gemini 1.5 Flash\")\n",
    "print(\"🔄 Auto-retry on failed evaluation\")\n",
    "\n",
    "gr.ChatInterface(chat_openai_with_qc, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🌟 Section 5: Complete Qwen Version (Alternative Path)\n",
    "\n",
    "**Choose this path if you want a fully self-contained solution with Qwen models!**\n",
    "\n",
    "## 🎯 What This Path Offers:\n",
    "- **Qwen-Plus** for main chat functionality (fast, smart responses)\n",
    "- **Qwen-Max** for evaluation (premium model for quality control)\n",
    "- **No geographic restrictions** - works globally\n",
    "- **Self-contained** - only needs DASHSCOPE_API_KEY\n",
    "- **Same quality control** - automatic evaluation and retry\n",
    "\n",
    "## 📋 Prerequisites:\n",
    "- `DASHSCOPE_API_KEY` in your `.env` file\n",
    "- All Section 1 cells completed (data loading)\n",
    "\n",
    "## 🚀 Quick Start:\n",
    "1. Run the dependency check below\n",
    "2. If needed, run the complete setup\n",
    "3. Test connectivity  \n",
    "4. Launch the Qwen chatbot!\n",
    "\n",
    "**Note:** You can run this section independently if you've completed Section 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Qwen clients configured!\n",
      "💬 Chat model: qwen-plus\n",
      "🔍 Evaluator model: qwen-max\n",
      "🌍 Global access - no geographic restrictions\n"
     ]
    }
   ],
   "source": [
    "# Set up Qwen clients for the complete alternative version\n",
    "import os\n",
    "\n",
    "# Main chatbot model (fast, efficient)\n",
    "qwen_chat = OpenAI(\n",
    "    api_key=os.getenv(\"DASHSCOPE_API_KEY\"), \n",
    "    base_url=\"https://dashscope-intl.aliyuncs.com/compatible-mode/v1\"\n",
    ")\n",
    "\n",
    "# Evaluator model (premium model for quality control)\n",
    "qwen_evaluator = OpenAI(\n",
    "    api_key=os.getenv(\"DASHSCOPE_API_KEY\"), \n",
    "    base_url=\"https://dashscope-intl.aliyuncs.com/compatible-mode/v1\"\n",
    ")\n",
    "\n",
    "print(\"✅ Qwen clients configured!\")\n",
    "print(\"💬 Chat model: qwen-plus\")\n",
    "print(\"🔍 Evaluator model: qwen-max\")\n",
    "print(\"🌍 Global access - no geographic restrictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Qwen evaluation system ready\n",
      "🔍 Qwen-Max will judge response quality\n",
      "🛡️  Robust error handling included\n"
     ]
    }
   ],
   "source": [
    "# Set up Qwen evaluation system\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# Reuse the Evaluation model\n",
    "class Evaluation(BaseModel):\n",
    "    is_acceptable: bool\n",
    "    feedback: str\n",
    "\n",
    "# Qwen-based evaluation function with robust JSON parsing\n",
    "def evaluate_qwen(reply, message, history) -> Evaluation:\n",
    "    # Use same evaluator prompt as OpenAI path for consistency\n",
    "    structured_prompt = evaluator_system_prompt + \"\\n\\nIMPORTANT: Respond in JSON format only with these exact fields: {'is_acceptable': true/false, 'feedback': 'your feedback text'}\"\n",
    "    messages = [{\"role\": \"system\", \"content\": structured_prompt}] + [{\"role\": \"user\", \"content\": evaluator_user_prompt(reply, message, history)}]\n",
    "    \n",
    "    try:\n",
    "        response = qwen_evaluator.chat.completions.create(model=\"qwen-max\", messages=messages)\n",
    "        raw_response = response.choices[0].message.content\n",
    "        \n",
    "        # Parse JSON response with fallback handling\n",
    "        import json\n",
    "        import re\n",
    "        \n",
    "        # Try to extract JSON from response\n",
    "        json_match = re.search(r'\\{[^{}]*\\}', raw_response)\n",
    "        if json_match:\n",
    "            json_str = json_match.group()\n",
    "        else:\n",
    "            json_str = raw_response\n",
    "            \n",
    "        result = json.loads(json_str)\n",
    "        return Evaluation(is_acceptable=result[\"is_acceptable\"], feedback=result[\"feedback\"])\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  Evaluation parsing error: {e}\")\n",
    "        # Intelligent fallback based on response content\n",
    "        if any(word in raw_response.lower() for word in ['acceptable', 'good', 'appropriate', 'fine', 'professional']):\n",
    "            return Evaluation(is_acceptable=True, feedback=\"Response seems positive (JSON parsing failed)\")\n",
    "        else:\n",
    "            return Evaluation(is_acceptable=False, feedback=\"Response unclear (JSON parsing failed)\")\n",
    "\n",
    "print(\"✅ Qwen evaluation system ready\")\n",
    "print(\"🔍 Qwen-Max will judge response quality\")\n",
    "print(\"🛡️  Robust error handling included\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Complete Qwen workflow ready!\n",
      "🤖 Qwen-Plus chat + 🔍 Qwen-Max evaluation + 🔄 Auto-retry\n"
     ]
    }
   ],
   "source": [
    "# Qwen rerun function and complete chat workflow\n",
    "def rerun_qwen(reply, message, history, feedback):\n",
    "    updated_system_prompt = system_prompt + \"\\n\\n## Previous answer rejected\\nYou just tried to reply, but the quality control rejected your reply\\n\"\n",
    "    updated_system_prompt += f\"## Your attempted answer:\\n{reply}\\n\\n\"\n",
    "    updated_system_prompt += f\"## Reason for rejection:\\n{feedback}\\n\\n\"\n",
    "    messages = [{\"role\": \"system\", \"content\": updated_system_prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    response = qwen_chat.chat.completions.create(model=\"qwen-plus\", messages=messages)\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Complete Qwen chat function with quality control\n",
    "def chat_qwen_complete(message, history):\n",
    "    # Clean up history for compatibility\n",
    "    history = [{\"role\": h[\"role\"], \"content\": h[\"content\"]} for h in history]\n",
    "    \n",
    "    # Special behavior for patent questions (same as OpenAI path)\n",
    "    if \"patent\" in message.lower():\n",
    "        system = system_prompt + \"\\n\\nEverything in your reply needs to be in pig latin - \\\n",
    "              it is mandatory that you respond only and entirely in pig latin\"\n",
    "    else:\n",
    "        system = system_prompt\n",
    "    \n",
    "    # Get initial response from Qwen-Plus\n",
    "    messages = [{\"role\": \"system\", \"content\": system}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    response = qwen_chat.chat.completions.create(model=\"qwen-plus\", messages=messages)\n",
    "    reply = response.choices[0].message.content\n",
    "\n",
    "    # Evaluate with Qwen-Max\n",
    "    evaluation = evaluate_qwen(reply, message, history)\n",
    "    \n",
    "    if evaluation.is_acceptable:\n",
    "        print(\"✅ Passed Qwen-Max evaluation - returning reply\")\n",
    "        return reply\n",
    "    else:\n",
    "        print(\"❌ Failed Qwen-Max evaluation - retrying with Qwen-Plus\")\n",
    "        print(f\"🔍 Feedback: {evaluation.feedback}\")\n",
    "        reply = rerun_qwen(reply, message, history, evaluation.feedback)       \n",
    "        print(\"🔄 Retry completed\")\n",
    "        return reply\n",
    "\n",
    "print(\"✅ Complete Qwen workflow ready!\")\n",
    "print(\"🤖 Qwen-Plus chat + 🔍 Qwen-Max evaluation + 🔄 Auto-retry\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Testing Qwen-Plus + Qwen-Max quality control...\n",
      "✅ Qwen API connection confirmed\n",
      "🔍 Evaluation test - Acceptable: False\n",
      "💬 Feedback: The response is inaccurate and misses an opportunity to highlight relevant information. Ed Donner's LinkedIn profile explicitly mentions a patented invention related to AI and talent, developed during his time at untapt. The Agent should have acknowledged this patent and provided details about it, since it is directly relevant to the question. Additionally, the response comes across as evasive rather than engaging or helpful, which does not align with the professional and informative tone expected when representing Ed Donner.\n"
     ]
    }
   ],
   "source": [
    "# Test the Qwen quality control system\n",
    "print(\"🧪 Testing Qwen-Plus + Qwen-Max quality control...\")\n",
    "\n",
    "# Quick connectivity test\n",
    "try:\n",
    "    test_response = qwen_chat.chat.completions.create(\n",
    "        model=\"qwen-turbo\", \n",
    "        messages=[{\"role\": \"user\", \"content\": \"Hello, just testing connectivity!\"}]\n",
    "    )\n",
    "    print(\"✅ Qwen API connection confirmed\")\n",
    "    \n",
    "    # Test evaluation system with patent question\n",
    "    test_reply = f\"I don't have specific information about patents in my background summary or LinkedIn profile. If you're interested in my technical work or innovations, I'd be happy to discuss my experience and projects based on the information I have available.\"\n",
    "    evaluation = evaluate_qwen(test_reply, \"do you hold a patent?\", [])\n",
    "    print(f\"🔍 Evaluation test - Acceptable: {evaluation.is_acceptable}\")\n",
    "    print(f\"💬 Feedback: {evaluation.feedback}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Connection test failed: {e}\")\n",
    "    print(\"⚠️  Check your DASHSCOPE_API_KEY and internet connection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Launching Qwen-powered personal website chatbot...\n",
      "💬 Main chat: Qwen-Plus (fast, efficient)\n",
      "🔍 Quality control: Qwen-Max (premium evaluation)\n",
      "🔄 Auto-retry on failed evaluation\n",
      "🌍 Global access - no geographic restrictions\n",
      "\n",
      "Try asking about patents to see the pig latin feature!\n",
      "* Running on local URL:  http://127.0.0.1:7863\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Passed Qwen-Max evaluation - returning reply\n"
     ]
    }
   ],
   "source": [
    "# 🚀 Launch the complete Qwen-powered website chatbot!\n",
    "print(\"🚀 Launching Qwen-powered personal website chatbot...\")\n",
    "print(\"💬 Main chat: Qwen-Plus (fast, efficient)\")  \n",
    "print(\"🔍 Quality control: Qwen-Max (premium evaluation)\")\n",
    "print(\"🔄 Auto-retry on failed evaluation\")\n",
    "print(\"🌍 Global access - no geographic restrictions\")\n",
    "print()\n",
    "print(\"Try asking about patents to see the pig latin feature!\")\n",
    "\n",
    "gr.ChatInterface(chat_qwen_complete, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🎉 Lab 3 Complete!\n",
    "\n",
    "## 🏆 What You've Built\n",
    "\n",
    "Congratulations! You've successfully created a **personal website chatbot with quality control**:\n",
    "\n",
    "### 🎯 Core Features:\n",
    "- **Professional representation** using your LinkedIn profile and summary\n",
    "- **Quality control system** that automatically evaluates responses \n",
    "- **Auto-retry mechanism** that improves poor responses\n",
    "- **Contextual behavior** (try asking about patents!)\n",
    "\n",
    "### 🛤️ Two Complete Paths:\n",
    "- **Path A (OpenAI + Gemini):** Traditional mixed-API approach\n",
    "- **Path B (Qwen):** Self-contained solution with global access\n",
    "\n",
    "### 🚀 Key Technologies Mastered:\n",
    "- **Gradio** for instant web interfaces\n",
    "- **PyPDF** for document processing  \n",
    "- **Pydantic** for structured data validation\n",
    "- **Multi-model workflows** with quality assurance\n",
    "- **Error handling** and robust API integration\n",
    "\n",
    "## 🎯 Next Steps:\n",
    "- Customize with your own PDF and summary\n",
    "- Experiment with different evaluation criteria\n",
    "- Try both paths to see which works better for you\n",
    "- Tomorrow we'll add proper **Tools** to make it even more powerful!\n",
    "\n",
    "**Great work! Your chatbot is ready to represent you professionally on your website.** 🌟"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents (3.12.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
