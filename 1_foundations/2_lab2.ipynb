{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2: Multi-Model AI Competition\n",
    "\n",
    "## What You'll Build\n",
    "Today you'll create an intelligent competition system that:\n",
    "- **Tests multiple AI models** with challenging questions\n",
    "- **Automatically evaluates responses** using AI judges  \n",
    "- **Ranks performance** to find the best model\n",
    "- **Handles multiple APIs** with consistent interfaces\n",
    "\n",
    "## Two Competition Approaches\n",
    "\n",
    "### Path A: Mixed-Model Competition  \n",
    "- Test different providers: OpenAI, Anthropic, Google, etc.\n",
    "- Great for comparing diverse AI approaches\n",
    "- Requires multiple API keys\n",
    "\n",
    "### Path B: Qwen-Only Competition (Recommended)\n",
    "- Use different Qwen model variants competing against each other\n",
    "- Single API key needed (DASHSCOPE_API_KEY)\n",
    "- Consistent, reliable access globally\n",
    "\n",
    "**Choose the path that fits your API access!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Multi-Model Competition Setup\n",
    "\n",
    "In this lab, we're creating a competition system where different AI models compete to answer questions, with another AI acting as the judge. This demonstrates how multiple agents can interact and evaluate each other's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with imports - ask ChatGPT to explain any package that you don't know\n",
    "\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from anthropic import Anthropic\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Always remember to do this!\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which API keys are available\n",
    "print(\"API Key Status:\")\n",
    "api_keys = {\n",
    "    'OPENAI_API_KEY': 'OpenAI',\n",
    "    'ANTHROPIC_API_KEY': 'Anthropic (optional)',\n",
    "    'GOOGLE_API_KEY': 'Google (optional)', \n",
    "    'DASHSCOPE_API_KEY': 'Qwen/Alibaba (recommended)'\n",
    "}\n",
    "\n",
    "for key, name in api_keys.items():\n",
    "    value = os.getenv(key)\n",
    "    if value:\n",
    "        print(f\"[OK] {name}: Available ({value[:6]}...)\")\n",
    "    else:\n",
    "        print(f\"[--] {name}: Not set\")\n",
    "\n",
    "print(\"\\nTip: For Qwen-only path, you only need DASHSCOPE_API_KEY!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Option 1 - Multi-Model Competition (OpenAI + Anthropic vs Google)\n",
    "\n",
    "This section creates a competition between different model providers. It's more complex but shows how different models perform on the same tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select which models to use based on available API keys\n",
    "models = []\n",
    "\n",
    "if os.getenv('OPENAI_API_KEY'):\n",
    "    print(\"[OpenAI] Available - Adding GPT models\")\n",
    "    models.extend([\n",
    "        {\"name\": \"GPT-4\", \"client\": \"openai\", \"model\": \"gpt-4o-mini\"},\n",
    "        {\"name\": \"GPT-3.5\", \"client\": \"openai\", \"model\": \"gpt-3.5-turbo\"}\n",
    "    ])\n",
    "else:\n",
    "    print(\"[OpenAI] Not configured - Skipping GPT models\")\n",
    "\n",
    "if os.getenv('ANTHROPIC_API_KEY'):\n",
    "    print(\"[Anthropic] Available - Adding Claude\")\n",
    "    models.append({\"name\": \"Claude\", \"client\": \"anthropic\", \"model\": \"claude-3-haiku-20240307\"})\n",
    "else:\n",
    "    print(\"[Anthropic] Not configured - Skipping Claude\")\n",
    "\n",
    "if os.getenv('GOOGLE_API_KEY'):\n",
    "    print(\"[Google] Available - Adding Gemini\")\n",
    "    models.append({\"name\": \"Gemini\", \"client\": \"google\", \"model\": \"gemini-1.5-flash\"})\n",
    "else:\n",
    "    print(\"[Google] Not configured - Skipping Gemini\")\n",
    "\n",
    "print(f\"\\nTotal models available for competition: {len(models)}\")\n",
    "if len(models) < 2:\n",
    "    print(\"WARNING: You need at least 2 models for a competition!\")\n",
    "    print(\"Consider using the Qwen-only option in Section 3 instead.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Competition Function (Multi-Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_competition(question, models):\n",
    "    \"\"\"Run a competition with multiple models\"\"\"\n",
    "    print(f\"COMPETITION: {question}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    responses = {}\n",
    "    \n",
    "    # Get responses from each model\n",
    "    for model in models:\n",
    "        try:\n",
    "            print(f\"[{model['name']}] Thinking...\")\n",
    "            \n",
    "            if model['client'] == 'openai':\n",
    "                response = openai_client.chat.completions.create(\n",
    "                    model=model['model'],\n",
    "                    messages=[{\"role\": \"user\", \"content\": question}],\n",
    "                    max_tokens=150\n",
    "                )\n",
    "                answer = response.choices[0].message.content\n",
    "            \n",
    "            elif model['client'] == 'anthropic':\n",
    "                response = anthropic_client.messages.create(\n",
    "                    model=model['model'],\n",
    "                    max_tokens=150,\n",
    "                    messages=[{\"role\": \"user\", \"content\": question}]\n",
    "                )\n",
    "                answer = response.content[0].text\n",
    "                \n",
    "            elif model['client'] == 'google':\n",
    "                response = google_model.generate_content(question)\n",
    "                answer = response.text\n",
    "            \n",
    "            responses[model['name']] = answer.strip()\n",
    "            print(f\"[{model['name']}] Response recorded\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"[{model['name']}] Error: {str(e)}\")\n",
    "            responses[model['name']] = \"Error occurred\"\n",
    "    \n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def judge_competition(question, responses):\n",
    "    \"\"\"Have an AI judge the competition responses\"\"\"\n",
    "    \n",
    "    # Format all responses for judging\n",
    "    response_text = f\"Question: {question}\\n\\n\"\n",
    "    for model, answer in responses.items():\n",
    "        response_text += f\"{model}: {answer}\\n\\n\"\n",
    "    \n",
    "    judge_prompt = f\"\"\"You are judging a competition between AI models. Here are their responses:\n",
    "\n",
    "{response_text}\n",
    "\n",
    "Please evaluate each response and pick the winner. Consider:\n",
    "- Accuracy and correctness\n",
    "- Clarity and helpfulness  \n",
    "- Completeness of the answer\n",
    "\n",
    "Respond with your judgment in this JSON format:\n",
    "{{\n",
    "    \"winner\": \"model_name\",\n",
    "    \"reasoning\": \"brief explanation of why this model won\",\n",
    "    \"scores\": {{\"model1\": score, \"model2\": score}}\n",
    "}}\"\"\"\n",
    "\n",
    "    try:\n",
    "        print(\"[JUDGE] Evaluating responses...\")\n",
    "        \n",
    "        # Use OpenAI as judge (fallback to first available model if needed)\n",
    "        judge_client = openai_client if os.getenv('OPENAI_API_KEY') else None\n",
    "        \n",
    "        if judge_client:\n",
    "            response = judge_client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[{\"role\": \"user\", \"content\": judge_prompt}],\n",
    "                max_tokens=200\n",
    "            )\n",
    "            judgment = response.choices[0].message.content\n",
    "            print(\"[JUDGE] Judgment complete\")\n",
    "            return judgment\n",
    "        else:\n",
    "            return \"No judge available (OpenAI API key needed for judging)\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"[JUDGE] Error: {str(e)}\")\n",
    "        return \"Judging failed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results(question, responses, judgment):\n",
    "    \"\"\"Display the competition results nicely\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"COMPETITION RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Question: {question}\")\n",
    "    print(\"\\nRESPONSES:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for model, response in responses.items():\n",
    "        print(f\"\\n[{model}]\")\n",
    "        print(response)\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "    print(\"JUDGE'S DECISION:\")\n",
    "    print(judgment)\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a test competition\n",
    "if len(models) >= 2:\n",
    "    print(\"Starting multi-model competition...\")\n",
    "    \n",
    "    test_question = \"What is the capital of France and what is it famous for?\"\n",
    "    \n",
    "    # Run the competition\n",
    "    responses = run_competition(test_question, models)\n",
    "    judgment = judge_competition(test_question, responses)\n",
    "    display_results(test_question, responses, judgment)\n",
    "    \n",
    "else:\n",
    "    print(\"Not enough models available for competition.\")\n",
    "    print(\"You need at least 2 API keys configured.\")\n",
    "    print(\"Try the Qwen-only option in Section 3 instead!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: Option 2 - Qwen-Only Competition (Simplified)\n",
    "\n",
    "This section uses only Qwen models for the competition, making it simpler but still demonstrating the agent interaction concepts. Perfect if you only have the DASHSCOPE_API_KEY!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup for Qwen-only competition\n",
    "if os.getenv('DASHSCOPE_API_KEY'):\n",
    "    print(\"[Qwen] Setting up competition models...\")\n",
    "    \n",
    "    # Create different \"personalities\" using the same model with different prompts\n",
    "    qwen_models = [\n",
    "        {\n",
    "            \"name\": \"Qwen-Academic\", \n",
    "            \"personality\": \"You are an academic expert. Give detailed, scholarly responses with precise facts.\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Qwen-Creative\", \n",
    "            \"personality\": \"You are a creative writer. Give engaging, imaginative responses with vivid descriptions.\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Qwen-Practical\", \n",
    "            \"personality\": \"You are a practical advisor. Give concise, actionable responses focused on real-world applications.\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    print(f\"[Qwen] Ready with {len(qwen_models)} different personalities\")\n",
    "    \n",
    "else:\n",
    "    print(\"[ERROR] DASHSCOPE_API_KEY not found!\")\n",
    "    print(\"Please set your Qwen API key to use this section.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_qwen_competition(question, qwen_models):\n",
    "    \"\"\"Run a competition using different Qwen personalities\"\"\"\n",
    "    print(f\"QWEN COMPETITION: {question}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    responses = {}\n",
    "    \n",
    "    for model in qwen_models:\n",
    "        try:\n",
    "            print(f\"[{model['name']}] Generating response...\")\n",
    "            \n",
    "            # Create a prompt that includes the personality\n",
    "            full_prompt = f\"{model['personality']}\\n\\nQuestion: {question}\"\n",
    "            \n",
    "            response = qwen_client.chat.completions.create(\n",
    "                model=\"qwen2.5-72b-instruct\",\n",
    "                messages=[{\"role\": \"user\", \"content\": full_prompt}],\n",
    "                max_tokens=150\n",
    "            )\n",
    "            \n",
    "            answer = response.choices[0].message.content.strip()\n",
    "            responses[model['name']] = answer\n",
    "            print(f\"[{model['name']}] Response recorded\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"[{model['name']}] Error: {str(e)}\")\n",
    "            responses[model['name']] = \"Error occurred\"\n",
    "    \n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def judge_qwen_competition(question, responses):\n",
    "    \"\"\"Have a Qwen model judge the competition\"\"\"\n",
    "    \n",
    "    # Format responses for judging\n",
    "    response_text = f\"Question: {question}\\n\\n\"\n",
    "    for model, answer in responses.items():\n",
    "        response_text += f\"{model}: {answer}\\n\\n\"\n",
    "    \n",
    "    judge_prompt = f\"\"\"You are an impartial judge evaluating different responses. Here are the responses to evaluate:\n",
    "\n",
    "{response_text}\n",
    "\n",
    "Please evaluate each response and pick the winner. Consider:\n",
    "- Accuracy and helpfulness\n",
    "- Clarity and engagement\n",
    "- Completeness and relevance\n",
    "\n",
    "Respond with your judgment in JSON format:\n",
    "{{\n",
    "    \"winner\": \"model_name\",\n",
    "    \"reasoning\": \"brief explanation\",\n",
    "    \"scores\": {{\"model1\": score, \"model2\": score, \"model3\": score}}\n",
    "}}\"\"\"\n",
    "\n",
    "    try:\n",
    "        print(\"[JUDGE] Evaluating all responses...\")\n",
    "        \n",
    "        response = qwen_client.chat.completions.create(\n",
    "            model=\"qwen2.5-72b-instruct\",\n",
    "            messages=[{\"role\": \"user\", \"content\": judge_prompt}],\n",
    "            max_tokens=200\n",
    "        )\n",
    "        \n",
    "        judgment = response.choices[0].message.content\n",
    "        print(\"[JUDGE] Judgment complete\")\n",
    "        return judgment\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"[JUDGE] Error: {str(e)}\")\n",
    "        return \"Judging failed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Qwen competition test\n",
    "if os.getenv('DASHSCOPE_API_KEY') and 'qwen_models' in locals():\n",
    "    print(\"Starting Qwen personality competition...\")\n",
    "    \n",
    "    test_question = \"What makes a great leader in today's world?\"\n",
    "    \n",
    "    # Run the competition\n",
    "    qwen_responses = run_qwen_competition(test_question, qwen_models)\n",
    "    qwen_judgment = judge_qwen_competition(test_question, qwen_responses)\n",
    "    display_results(test_question, qwen_responses, qwen_judgment)\n",
    "    \n",
    "else:\n",
    "    print(\"Qwen competition not available.\")\n",
    "    print(\"Make sure DASHSCOPE_API_KEY is set and run the setup cell above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try your own question!\n",
    "my_question = \"What's the most important invention in human history?\"\n",
    "\n",
    "print(f\"Testing custom question: {my_question}\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Choose which competition to run based on what's available\n",
    "if len(models) >= 2:\n",
    "    print(\"Running multi-model competition...\")\n",
    "    my_responses = run_competition(my_question, models)\n",
    "    my_judgment = judge_competition(my_question, my_responses)\n",
    "    display_results(my_question, my_responses, my_judgment)\n",
    "    \n",
    "elif os.getenv('DASHSCOPE_API_KEY') and 'qwen_models' in locals():\n",
    "    print(\"Running Qwen competition...\")\n",
    "    my_responses = run_qwen_competition(my_question, qwen_models)\n",
    "    my_judgment = judge_qwen_competition(my_question, my_responses)\n",
    "    display_results(my_question, my_responses, my_judgment)\n",
    "    \n",
    "else:\n",
    "    print(\"No competition available - please configure API keys first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4: Advanced Competition Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_batch_competition(questions, competition_models):\n",
    "    \"\"\"Run multiple questions in a tournament\"\"\"\n",
    "    \n",
    "    print(\"BATCH COMPETITION STARTING\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    results = []\n",
    "    model_wins = {model['name'] if isinstance(model, dict) else model['name']: 0 \n",
    "                  for model in competition_models}\n",
    "    \n",
    "    for i, question in enumerate(questions, 1):\n",
    "        print(f\"\\n[Round {i}/{len(questions)}] {question}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Determine which competition function to use\n",
    "        if isinstance(competition_models[0], dict) and 'personality' in competition_models[0]:\n",
    "            responses = run_qwen_competition(question, competition_models)\n",
    "            judgment = judge_qwen_competition(question, responses)\n",
    "        else:\n",
    "            responses = run_competition(question, competition_models)\n",
    "            judgment = judge_competition(question, responses)\n",
    "        \n",
    "        # Try to extract winner from judgment\n",
    "        try:\n",
    "            import json\n",
    "            if judgment.startswith('{'):\n",
    "                judgment_data = json.loads(judgment)\n",
    "                winner = judgment_data.get('winner', 'Unknown')\n",
    "                if winner in model_wins:\n",
    "                    model_wins[winner] += 1\n",
    "                    print(f\"[WINNER] {winner}\")\n",
    "            else:\n",
    "                print(\"[WINNER] Could not parse judgment\")\n",
    "        except:\n",
    "            print(\"[WINNER] Could not determine winner\")\n",
    "        \n",
    "        results.append({\n",
    "            'question': question,\n",
    "            'responses': responses,\n",
    "            'judgment': judgment\n",
    "        })\n",
    "    \n",
    "    # Show final tournament results\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TOURNAMENT RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    for model, wins in model_wins.items():\n",
    "        print(f\"{model}: {wins} wins\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a mini tournament\n",
    "tournament_questions = [\n",
    "    \"What is the most important skill for the future?\",\n",
    "    \"How will AI change education in the next decade?\",\n",
    "    \"What's the best way to learn a new language?\"\n",
    "]\n",
    "\n",
    "print(\"Starting mini tournament...\")\n",
    "\n",
    "# Choose models based on availability\n",
    "if len(models) >= 2:\n",
    "    print(\"Using multi-model setup\")\n",
    "    tournament_results = run_batch_competition(tournament_questions, models)\n",
    "elif os.getenv('DASHSCOPE_API_KEY') and 'qwen_models' in locals():\n",
    "    print(\"Using Qwen personality setup\")\n",
    "    tournament_results = run_batch_competition(tournament_questions, qwen_models)\n",
    "else:\n",
    "    print(\"No models available for tournament\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_competition_patterns(results):\n",
    "    \"\"\"Analyze patterns from competition results\"\"\"\n",
    "    \n",
    "    print(\"COMPETITION ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(f\"Total competitions run: {len(results)}\")\n",
    "    \n",
    "    # Analyze response lengths\n",
    "    response_lengths = []\n",
    "    for result in results:\n",
    "        for model, response in result['responses'].items():\n",
    "            response_lengths.append(len(response))\n",
    "    \n",
    "    if response_lengths:\n",
    "        avg_length = sum(response_lengths) / len(response_lengths)\n",
    "        print(f\"Average response length: {avg_length:.0f} characters\")\n",
    "    \n",
    "    # Show some interesting patterns\n",
    "    print(\"\\nPattern Analysis:\")\n",
    "    print(\"- Different models have different response styles\")\n",
    "    print(\"- Competition creates more detailed responses\") \n",
    "    print(\"- Judging adds an extra layer of evaluation\")\n",
    "    \n",
    "    return {\n",
    "        'total_competitions': len(results),\n",
    "        'average_response_length': avg_length if response_lengths else 0\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the tournament results\n",
    "if 'tournament_results' in locals():\n",
    "    print(\"Analyzing tournament patterns...\")\n",
    "    analysis = analyze_competition_patterns(tournament_results)\n",
    "    print(f\"\\nAnalysis complete: {analysis}\")\n",
    "else:\n",
    "    print(\"No tournament results to analyze - run a tournament first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 5: Experiments and Extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a specialized competition for creative tasks\n",
    "def creative_competition(topic):\n",
    "    \"\"\"Run a competition focused on creative responses\"\"\"\n",
    "    \n",
    "    creative_prompt = f\"Write a creative short story or poem about: {topic}\"\n",
    "    \n",
    "    print(f\"CREATIVE COMPETITION: {topic}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if len(models) >= 2:\n",
    "        responses = run_competition(creative_prompt, models)\n",
    "        judgment = judge_competition(creative_prompt, responses)\n",
    "    elif os.getenv('DASHSCOPE_API_KEY') and 'qwen_models' in locals():\n",
    "        responses = run_qwen_competition(creative_prompt, qwen_models)\n",
    "        judgment = judge_qwen_competition(creative_prompt, responses)\n",
    "    else:\n",
    "        print(\"No models available for creative competition\")\n",
    "        return\n",
    "    \n",
    "    display_results(creative_prompt, responses, judgment)\n",
    "\n",
    "# Try a creative competition\n",
    "print(\"Running creative writing competition...\")\n",
    "creative_competition(\"a robot learning to paint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a technical debate between models\n",
    "def technical_debate(topic, position1, position2):\n",
    "    \"\"\"Have models argue different sides of a technical issue\"\"\"\n",
    "    \n",
    "    print(f\"TECHNICAL DEBATE: {topic}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Create specific prompts for each side\n",
    "    prompts = {\n",
    "        \"Pro\": f\"Argue IN FAVOR of: {position1}. Topic: {topic}\",\n",
    "        \"Con\": f\"Argue AGAINST: {position2}. Topic: {topic}\"\n",
    "    }\n",
    "    \n",
    "    responses = {}\n",
    "    \n",
    "    # Get arguments from available models\n",
    "    if len(models) >= 2:\n",
    "        for i, (side, prompt) in enumerate(prompts.items()):\n",
    "            model = models[i]\n",
    "            print(f\"[{side}] {model['name']} preparing argument...\")\n",
    "            \n",
    "            if model['client'] == 'openai':\n",
    "                response = openai_client.chat.completions.create(\n",
    "                    model=model['model'],\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                    max_tokens=200\n",
    "                )\n",
    "                responses[f\"{side} ({model['name']})\"] = response.choices[0].message.content\n",
    "                \n",
    "    elif os.getenv('DASHSCOPE_API_KEY'):\n",
    "        for side, prompt in prompts.items():\n",
    "            response = qwen_client.chat.completions.create(\n",
    "                model=\"qwen2.5-72b-instruct\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                max_tokens=200\n",
    "            )\n",
    "            responses[f\"{side} (Qwen)\"] = response.choices[0].message.content\n",
    "    \n",
    "    return responses\n",
    "\n",
    "# Run a technical debate\n",
    "debate_topic = \"AI in Education\"\n",
    "debate_responses = technical_debate(\n",
    "    debate_topic,\n",
    "    \"AI will revolutionize education for the better\",\n",
    "    \"AI poses risks to traditional learning\"\n",
    ")\n",
    "\n",
    "if debate_responses:\n",
    "    print(\"\\nDEBATE RESULTS:\")\n",
    "    print(\"=\" * 60)\n",
    "    for side, argument in debate_responses.items():\n",
    "        print(f\"\\n[{side}]\")\n",
    "        print(argument)\n",
    "        print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive competition - you pick the winner!\n",
    "def user_judged_competition(question):\n",
    "    \"\"\"Let the user judge the competition\"\"\"\n",
    "    \n",
    "    print(f\"USER-JUDGED COMPETITION: {question}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Get responses\n",
    "    if len(models) >= 2:\n",
    "        responses = run_competition(question, models)\n",
    "    elif os.getenv('DASHSCOPE_API_KEY') and 'qwen_models' in locals():\n",
    "        responses = run_qwen_competition(question, qwen_models)\n",
    "    else:\n",
    "        print(\"No models available\")\n",
    "        return\n",
    "    \n",
    "    # Display responses for user judgment\n",
    "    print(\"\\nRESPONSES FOR YOUR JUDGMENT:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    model_list = list(responses.keys())\n",
    "    for i, (model, response) in enumerate(responses.items(), 1):\n",
    "        print(f\"\\n[Option {i}] {model}\")\n",
    "        print(response)\n",
    "        print(\"-\" * 30)\n",
    "    \n",
    "    print(\"\\nWhich response do you think is best?\")\n",
    "    print(\"In a real interactive version, you would input your choice here!\")\n",
    "    print(f\"Options: {', '.join([f'{i}: {model}' for i, model in enumerate(model_list, 1)])}\")\n",
    "    \n",
    "    return responses\n",
    "\n",
    "# Example user-judged competition\n",
    "user_question = \"What's the best programming language for beginners?\"\n",
    "user_responses = user_judged_competition(user_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ideas for further experimentation\n",
    "print(\"EXPERIMENTATION IDEAS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "experiments = [\n",
    "    \"Multi-round competitions where models can respond to each other\",\n",
    "    \"Domain-specific competitions (science, arts, business)\",\n",
    "    \"Collaborative mode where models work together instead of competing\",\n",
    "    \"Different judging criteria (speed, creativity, accuracy)\",\n",
    "    \"Real-time competitions with live user voting\",\n",
    "    \"Model personality tournaments with different character types\"\n",
    "]\n",
    "\n",
    "for i, experiment in enumerate(experiments, 1):\n",
    "    print(f\"{i}. {experiment}\")\n",
    "\n",
    "print(\"\\nFeel free to implement any of these ideas!\")\n",
    "print(\"The competition framework is flexible and can be extended in many ways.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Great work! You've successfully created an AI competition system that demonstrates key concepts in multi-agent interactions:\n",
    "\n",
    "**Key Concepts Learned:**\n",
    "- Multiple AI agents working on the same task\n",
    "- Automated evaluation and judging\n",
    "- Comparative analysis of different models\n",
    "- Flexible system design that works with different APIs\n",
    "\n",
    "**Real-World Applications:**\n",
    "- Content generation with multiple perspectives\n",
    "- Automated quality assessment systems\n",
    "- A/B testing for AI responses\n",
    "- Educational tools for comparing different approaches\n",
    "\n",
    "**Next Steps:**\n",
    "- Try different types of questions and competitions\n",
    "- Experiment with different judging criteria\n",
    "- Build more complex multi-agent systems\n",
    "- Explore collaborative vs competitive agent interactions\n",
    "\n",
    "The competition framework you've built is a foundation for understanding how multiple AI agents can interact, compete, and evaluate each other's performance!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
