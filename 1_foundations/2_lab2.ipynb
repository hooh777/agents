{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üèÜ Lab 2: Multi-Model AI Competition\n",
    "\n",
    "## üéØ What You'll Build\n",
    "Today you'll create an intelligent competition system that:\n",
    "- **Tests multiple AI models** with challenging questions\n",
    "- **Automatically evaluates responses** using AI judges  \n",
    "- **Ranks performance** to find the best model\n",
    "- **Handles multiple APIs** with consistent interfaces\n",
    "\n",
    "## üõ§Ô∏è Two Competition Approaches\n",
    "\n",
    "### Path A: Mixed-Model Competition  \n",
    "- Test different providers: OpenAI, Anthropic, Google, etc.\n",
    "- Great for comparing diverse AI approaches\n",
    "- Requires multiple API keys\n",
    "\n",
    "### Path B: Qwen-Only Competition (Recommended)\n",
    "- Use different Qwen model variants competing against each other\n",
    "- Single API key needed (DASHSCOPE_API_KEY)\n",
    "- Consistent, reliable access globally\n",
    "\n",
    "**Choose the path that fits your API access!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/stop.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#ff7800;\">Important point - please read</h2>\n",
    "            <span style=\"color:#ff7800;\">The way I collaborate with you may be different to other courses you've taken. I prefer not to type code while you watch. Rather, I execute Jupyter Labs, like this, and give you an intuition for what's going on. My suggestion is that you carefully execute this yourself, <b>after</b> watching the lecture. Add print statements to understand what's going on, and then come up with your own variations.<br/><br/>If you have time, I'd love it if you submit a PR for changes in the community_contributions folder - instructions in the resources. Also, if you have a Github account, use this to showcase your variations. Not only is this essential practice, but it demonstrates your skills to others, including perhaps future clients or employers...\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with imports - ask ChatGPT to explain any package that you don't know\n",
    "\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from anthropic import Anthropic\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Always remember to do this!\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which API keys are available\n",
    "print(\"üîë API Key Status:\")\n",
    "api_keys = {\n",
    "    'OPENAI_API_KEY': 'OpenAI',\n",
    "    'ANTHROPIC_API_KEY': 'Anthropic (optional)',\n",
    "    'GOOGLE_API_KEY': 'Google (optional)', \n",
    "    'DASHSCOPE_API_KEY': 'Qwen/Alibaba (recommended)'\n",
    "}\n",
    "\n",
    "for key, name in api_keys.items():\n",
    "    value = os.getenv(key)\n",
    "    if value:\n",
    "        print(f\"‚úÖ {name}: Available ({value[:6]}...)\")\n",
    "    else:\n",
    "        print(f\"‚ùå {name}: Not set\")\n",
    "\n",
    "print(\"\\nüí° Tip: For Qwen-only path, you only need DASHSCOPE_API_KEY!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìö Section 1: Generate Challenge Question\n",
    "\n",
    "First, we'll create a challenging question to test the AI models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the challenge question using OpenAI\n",
    "request = \"Please come up with a challenging, nuanced question that I can ask a number of LLMs to evaluate their intelligence. \"\n",
    "request += \"Answer only with the question, no explanation.\"\n",
    "messages = [{\"role\": \"user\", \"content\": request}]\n",
    "\n",
    "openai = OpenAI()\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=messages,\n",
    ")\n",
    "question = response.choices[0].message.content\n",
    "print(\"üéØ Challenge Question Generated:\")\n",
    "print(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìö Section 2: Mixed-Model Competition (Path A)\n",
    "\n",
    "**Only run this section if you have multiple API keys and want to test different providers!**\n",
    "\n",
    "If you prefer a simpler approach with just Qwen, skip to Section 3 below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize competition tracking\n",
    "competitors = []\n",
    "answers = []\n",
    "test_messages = [{\"role\": \"user\", \"content\": question}]\n",
    "\n",
    "# Competitor 1: OpenAI GPT-4o-mini\n",
    "model_name = \"gpt-4o-mini\"\n",
    "response = openai.chat.completions.create(model=model_name, messages=test_messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(f\"**{model_name}:**\"))\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Competitor 2: Anthropic Claude\n",
    "model_name = \"claude-3-5-sonnet-20241022\"\n",
    "\n",
    "claude = Anthropic()\n",
    "response = claude.messages.create(model=model_name, messages=test_messages, max_tokens=1000)\n",
    "answer = response.content[0].text\n",
    "\n",
    "display(Markdown(f\"**{model_name}:**\"))\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Competitor 3: Google Gemini  \n",
    "gemini = OpenAI(\n",
    "    api_key=os.getenv(\"GOOGLE_API_KEY\"), \n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")\n",
    "model_name = \"gemini-2.0-flash\"\n",
    "\n",
    "response = gemini.chat.completions.create(model=model_name, messages=test_messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(f\"**{model_name}:**\"))\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Competitor 4: Qwen (Alibaba Cloud)\n",
    "qwen = OpenAI(\n",
    "    api_key=os.getenv(\"DASHSCOPE_API_KEY\"), \n",
    "    base_url=\"https://dashscope-intl.aliyuncs.com/compatible-mode/v1\"\n",
    ")\n",
    "model_name = \"qwen-plus\"\n",
    "\n",
    "response = qwen.chat.completions.create(model=model_name, messages=test_messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(f\"**{model_name}:**\"))\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üåü Section 3: Complete Qwen-Only Competition (Path B - Recommended)\n",
    "\n",
    "**Choose this path for a simpler, more reliable approach!**\n",
    "\n",
    "This section creates a competition between different Qwen model variants:\n",
    "- **Qwen-Max** (Most powerful)\n",
    "- **Qwen-Plus** (Balanced performance/cost) \n",
    "- **Qwen-Turbo** (Fast and efficient)\n",
    "- **Qwen-Long** (Specialized for long contexts)\n",
    "\n",
    "**Benefits:**\n",
    "- Single API key needed (DASHSCOPE_API_KEY)\n",
    "- Consistent performance and access\n",
    "- No geographic restrictions\n",
    "- Cost-effective testing\n",
    "\n",
    "**Setup:** Make sure `DASHSCOPE_API_KEY` is in your `.env` file from https://modelstudio.console.alibabacloud.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Qwen client and generate question\n",
    "qwen_client = OpenAI(\n",
    "    api_key=os.getenv('DASHSCOPE_API_KEY'), \n",
    "    base_url=\"https://dashscope-intl.aliyuncs.com/compatible-mode/v1\"\n",
    ")\n",
    "\n",
    "# Create question using Qwen-Max (most powerful model)\n",
    "request = \"Please come up with a challenging, nuanced question that I can ask a number of LLMs to evaluate their intelligence. \"\n",
    "request += \"Answer only with the question, no explanation.\"\n",
    "\n",
    "response = qwen_client.chat.completions.create(\n",
    "    model=\"qwen-max\",\n",
    "    messages=[{\"role\": \"user\", \"content\": request}]\n",
    ")\n",
    "qwen_question = response.choices[0].message.content\n",
    "\n",
    "print(\"üéØ Question generated by Qwen-Max:\")\n",
    "print(qwen_question)\n",
    "\n",
    "# Initialize Qwen competition\n",
    "qwen_competitors = []\n",
    "qwen_answers = []\n",
    "qwen_messages = [{\"role\": \"user\", \"content\": qwen_question}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qwen Competitor 1: Qwen-Max (Most powerful)\n",
    "model_name = \"qwen-max\"\n",
    "response = qwen_client.chat.completions.create(model=model_name, messages=qwen_messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(f\"**{model_name}:**\"))\n",
    "display(Markdown(answer))\n",
    "qwen_competitors.append(model_name)\n",
    "qwen_answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qwen Competitor 2: Qwen-Plus (Balanced)\n",
    "model_name = \"qwen-plus\" \n",
    "response = qwen_client.chat.completions.create(model=model_name, messages=qwen_messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(f\"**{model_name}:**\"))\n",
    "display(Markdown(answer))\n",
    "qwen_competitors.append(model_name)\n",
    "qwen_answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qwen Competitor 3: Qwen-Turbo (Fast)\n",
    "model_name = \"qwen-turbo\"\n",
    "response = qwen_client.chat.completions.create(model=model_name, messages=qwen_messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(f\"**{model_name}:**\"))\n",
    "display(Markdown(answer))\n",
    "qwen_competitors.append(model_name)\n",
    "qwen_answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qwen Competitor 4: Qwen-Long (Long context)\n",
    "model_name = \"qwen-long\"\n",
    "response = qwen_client.chat.completions.create(model=model_name, messages=qwen_messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(f\"**{model_name}:**\"))\n",
    "display(Markdown(answer))\n",
    "qwen_competitors.append(model_name)\n",
    "qwen_answers.append(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üèõÔ∏è Section 4: Qwen Competition Judging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile all Qwen answers for judging\n",
    "print(f\"üìä Qwen Competition Status: {len(qwen_competitors)} competitors, {len(qwen_answers)} answers\")\n",
    "\n",
    "qwen_together = \"\"\n",
    "for index, answer in enumerate(qwen_answers):\n",
    "    qwen_together += f\"# Response from Qwen competitor {index+1}\\n\\n\"\n",
    "    qwen_together += answer + \"\\n\\n\"\n",
    "\n",
    "print(\"‚úÖ All responses compiled for judging!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create judge prompt for Qwen competition\n",
    "qwen_judge_prompt = f\"\"\"You are judging a competition between {len(qwen_competitors)} different Qwen model variants.\n",
    "\n",
    "Question asked: {qwen_question}\n",
    "\n",
    "Your job is to evaluate each response for clarity and strength of argument, and rank them from best to worst.\n",
    "Respond with JSON only: {{\"results\": [\"best competitor number\", \"second best\", \"third best\", ...]}}\n",
    "\n",
    "Responses from each Qwen competitor:\n",
    "{qwen_together}\n",
    "\n",
    "Respond with ONLY the JSON ranking, no markdown or explanations.\"\"\"\n",
    "\n",
    "print(\"‚öñÔ∏è Judge prompt created for Qwen competition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute Qwen competition judging\n",
    "judge_messages = [{\"role\": \"user\", \"content\": qwen_judge_prompt}]\n",
    "response = qwen_client.chat.completions.create(\n",
    "    model=\"qwen-max\",  # Use most powerful model as judge\n",
    "    messages=judge_messages\n",
    ")\n",
    "\n",
    "qwen_results = response.choices[0].message.content\n",
    "print(\"‚öñÔ∏è Qwen-Max judgment:\")\n",
    "print(qwen_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Qwen competition results\n",
    "qwen_results_dict = json.loads(qwen_results)\n",
    "qwen_ranks = qwen_results_dict[\"results\"]\n",
    "\n",
    "print(\"üèÜ QWEN COMPETITION FINAL RESULTS:\")\n",
    "print(\"=\" * 40)\n",
    "for index, result in enumerate(qwen_ranks):\n",
    "    competitor = qwen_competitors[int(result)-1]\n",
    "    print(f\"ü•á Rank {index+1}: {competitor}\")\n",
    "\n",
    "print(f\"\\nüéâ Qwen competition complete! {len(qwen_competitors)} models evaluated by Qwen-Max\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üèõÔ∏è Section 5: Mixed-Model Competition Judging (Path A)\n",
    "\n",
    "**Only run this section if you completed Section 2 (Mixed-Model Competition)!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check mixed-model competition status and compile answers\n",
    "print(f\"üìä Mixed-Model Competition Status:\")\n",
    "print(f\"Competitors: {competitors}\")\n",
    "print(f\"Answers collected: {len(answers)}\")\n",
    "\n",
    "# Compile all answers for judging\n",
    "together = \"\"\n",
    "for index, answer in enumerate(answers):\n",
    "    together += f\"# Response from competitor {index+1}\\n\\n\"\n",
    "    together += answer + \"\\n\\n\"\n",
    "\n",
    "print(\"‚úÖ All mixed-model responses compiled!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create judge prompt for mixed-model competition  \n",
    "judge_prompt = f\"\"\"You are judging a competition between {len(competitors)} AI models.\n",
    "\n",
    "Question asked: {question}\n",
    "\n",
    "Your job is to evaluate each response for clarity and strength of argument, and rank them from best to worst.\n",
    "Respond with JSON only: {{\"results\": [\"best competitor number\", \"second best\", \"third best\", ...]}}\n",
    "\n",
    "Responses from each competitor:\n",
    "{together}\n",
    "\n",
    "Respond with ONLY the JSON ranking, no markdown or explanations.\"\"\"\n",
    "\n",
    "print(\"‚öñÔ∏è Judge prompt created for mixed-model competition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute mixed-model competition judging\n",
    "judge_messages = [{\"role\": \"user\", \"content\": judge_prompt}]\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=judge_messages\n",
    ")\n",
    "\n",
    "results = response.choices[0].message.content\n",
    "print(\"‚öñÔ∏è GPT-4o-mini judgment:\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display mixed-model competition results\n",
    "results_dict = json.loads(results)\n",
    "ranks = results_dict[\"results\"]\n",
    "\n",
    "print(\"üèÜ MIXED-MODEL COMPETITION FINAL RESULTS:\")\n",
    "print(\"=\" * 45)\n",
    "for index, result in enumerate(ranks):\n",
    "    competitor = competitors[int(result)-1]\n",
    "    print(f\"ü•á Rank {index+1}: {competitor}\")\n",
    "\n",
    "print(f\"\\nüéâ Mixed-model competition complete! {len(competitors)} models evaluated by GPT-4o-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ Lab 2 Complete!\n",
    "\n",
    "## üèÜ What You've Built\n",
    "\n",
    "You've successfully created a sophisticated **AI model competition system**:\n",
    "\n",
    "### üöÄ Core Features:\n",
    "- **Multi-model testing** - Compare different AI providers or model variants\n",
    "- **Automated evaluation** - AI judges rank performance objectively  \n",
    "- **Flexible competition formats** - Mixed models or single-provider variants\n",
    "- **JSON-structured results** - Clean, parseable output format\n",
    "\n",
    "### üõ§Ô∏è Two Paths Available:\n",
    "- **Path A (Mixed-Model):** Test OpenAI, Anthropic, Google, Qwen together\n",
    "- **Path B (Qwen-Only):** Compare Qwen model variants (simpler, more reliable)\n",
    "\n",
    "### üîß Key Technologies Mastered:\n",
    "- **Multiple API integrations** with consistent OpenAI-compatible interfaces\n",
    "- **Automated judging systems** using AI as evaluators\n",
    "- **JSON parsing and results processing**\n",
    "- **Competition workflow orchestration**\n",
    "\n",
    "## üéØ Agentic Patterns Used:\n",
    "- **Multi-agent evaluation** - Different models compete and judge\n",
    "- **Structured output processing** - JSON responses for reliable parsing\n",
    "- **Workflow orchestration** - Systematic testing and evaluation pipeline\n",
    "\n",
    "## üíº Commercial Applications:\n",
    "These patterns are valuable for:\n",
    "- **Model selection** for production systems\n",
    "- **Performance benchmarking** across different AI providers  \n",
    "- **Quality assurance** for AI-powered applications\n",
    "- **Cost-performance optimization** by comparing model effectiveness\n",
    "\n",
    "**Great work! You now have a robust system for evaluating and comparing AI models.** üåü"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
