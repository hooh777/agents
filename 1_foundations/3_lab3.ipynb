{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: Personal Website Chatbot with Quality Control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Objectives\n",
    "\n",
    "In this lab, you'll build an intelligent chatbot that can answer questions about a personal website or portfolio. The chatbot will include quality control features to ensure helpful responses.\n",
    "\n",
    "**What you'll learn:**\n",
    "- Building domain-specific chatbots\n",
    "- Quality control and evaluation systems\n",
    "- PDF processing and content extraction\n",
    "- Advanced prompting techniques\n",
    "- Multi-model comparison approaches\n",
    "\n",
    "**What you'll build:**\n",
    "- A chatbot that knows about your personal/professional content\n",
    "- Quality evaluation system for chatbot responses  \n",
    "- A simple web interface using Gradio\n",
    "- Comparison between different AI models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Environment Setup and Content Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel\n",
    "from typing import Optional\n",
    "import pypdf\n",
    "import gradio as gr\n",
    "\n",
    "print(\"Starting Lab 3: Personal Website Chatbot\")\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Environment variables loaded\n",
      "üìã Available APIs will depend on your chosen path\n"
     ]
    }
   ],
   "source": [
    "# Check available API keys\n",
    "required_vars = ['OPENAI_API_KEY', 'GOOGLE_API_KEY', 'DASHSCOPE_API_KEY']\n",
    "missing_vars = []\n",
    "\n",
    "print(\"API Key Status Check:\")\n",
    "for var in required_vars:\n",
    "    if os.getenv(var):\n",
    "        print(f\"[OK] {var}: Available\")\n",
    "    else:\n",
    "        print(f\"[--] {var}: Not set\")\n",
    "        missing_vars.append(var)\n",
    "\n",
    "if missing_vars:\n",
    "    print(f\"\\nMissing API keys: {', '.join(missing_vars)}\")\n",
    "    print(\"You can still run this lab with just one API key!\")\n",
    "else:\n",
    "    print(\"\\nAll API keys available - you can use any path!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LinkedIn profile loaded (8256 characters)\n",
      "üìÑ Replace me/linkedin.pdf with your own profile!\n"
     ]
    }
   ],
   "source": [
    "# Load your personal/professional content from PDF\n",
    "# Replace 'your_resume.pdf' with your actual PDF file path\n",
    "\n",
    "pdf_path = 'your_resume.pdf'  # Change this to your PDF file\n",
    "\n",
    "try:\n",
    "    with open(pdf_path, 'rb') as f:\n",
    "        reader = pypdf.PdfReader(f)\n",
    "        text = \"\"\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text()\n",
    "    \n",
    "    print(f\"[SUCCESS] PDF loaded: {len(text)} characters extracted\")\n",
    "    print(f\"First 200 characters: {text[:200]}...\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"[INFO] PDF file not found - using sample content instead\")\n",
    "    \n",
    "    # Sample professional content if no PDF is available\n",
    "    text = \"\"\"\n",
    "    John Smith - Software Developer\n",
    "    \n",
    "    Professional Experience:\n",
    "    - 5 years of experience in Python development\n",
    "    - Specialized in AI/ML applications and web development\n",
    "    - Led teams of 3-5 developers on multiple projects\n",
    "    - Experience with OpenAI API, machine learning frameworks\n",
    "    \n",
    "    Skills:\n",
    "    - Programming: Python, JavaScript, SQL\n",
    "    - AI/ML: OpenAI API, scikit-learn, TensorFlow\n",
    "    - Web: React, Flask, Django\n",
    "    - Tools: Git, Docker, AWS\n",
    "    \n",
    "    Education:\n",
    "    - Bachelor's in Computer Science\n",
    "    - Certified in Cloud Computing\n",
    "    \n",
    "    Projects:\n",
    "    - Built an AI-powered chatbot for customer service\n",
    "    - Developed a machine learning model for predictive analytics  \n",
    "    - Created a full-stack web application with 10,000+ users\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"[INFO] Using sample content for demonstration\")\n",
    "\n",
    "print(f\"Total content length: {len(text)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¬† ¬†\n",
      "Contact\n",
      "ed.donner@gmail.com\n",
      "www.linkedin.com/in/eddonner\n",
      "(LinkedIn)\n",
      "edwarddonner.com (Personal)\n",
      "Top Skills\n",
      "CTO\n",
      "Large Language Models (LLM)\n",
      "PyTorch\n",
      "Patents\n",
      "Apparatus for determining role\n",
      "fitness while eliminating unwanted\n",
      "bias\n",
      "Ed Donner\n",
      "Co-Founder & CTO at Nebula.io, repeat Co-Founder of AI startups,\n",
      "speaker & advisor on Gen AI and LLM Engineering\n",
      "New York, New York, United States\n",
      "Summary\n",
      "I‚Äôm a technology leader and entrepreneur. I'm applying AI to a field\n",
      "where it can make a massive impact: helping people discover their\n",
      "potential and pursue their reason for being. But at my core, I‚Äôm a\n",
      "software engineer and a scientist. I learned how to code aged 8 and\n",
      "still spend weekends experimenting with Large Language Models\n",
      "and writing code (rather badly). If you‚Äôd like to join us to show me\n",
      "how it‚Äôs done.. message me!\n",
      "As a work-hobby, I absolutely love giving talks about Gen AI and\n",
      "LLMs. I'm the author of a best-selling, top-rated Udemy course\n",
      "on LLM Engineering, and I speak at O'Reilly Live Events and\n",
      "ODSC workshops. It brings me great joy to help others unlock the\n",
      "astonishing power of LLMs.\n",
      "I spent most of my career at JPMorgan building software for financial\n",
      "markets. I worked in London, Tokyo and New York. I became an MD\n",
      "running a global organization of 300. Then I left to start my own AI\n",
      "business, untapt, to solve the problem that had plagued me at JPM -\n",
      "why is so hard to hire engineers?\n",
      "At untapt we worked with GQR, one of the world's fastest growing\n",
      "recruitment firms. We collaborated on a patented invention in AI\n",
      "and talent. Our skills were perfectly complementary - AI leaders vs\n",
      "recruitment leaders - so much so, that we decided to join forces. In\n",
      "2020, untapt was acquired by GQR‚Äôs parent company and Nebula\n",
      "was born.\n",
      "I‚Äôm now Co-Founder and CTO for Nebula, responsible for software\n",
      "engineering and data science.  Our stack is Python/Flask, React,\n",
      "Mongo, ElasticSearch, with Kubernetes on GCP. Our 'secret sauce'\n",
      "is our use of Gen AI and proprietary LLMs. If any of this sounds\n",
      "interesting - we should talk!\n",
      "¬† Page 1 of 5¬† ¬†\n",
      "Experience\n",
      "Nebula.io\n",
      "Co-Founder & CTO\n",
      "June 2021¬†-¬†Present¬†(3 years 10 months)\n",
      "New York, New York, United States\n",
      "I‚Äôm the co-founder and CTO of Nebula.io. We help recruiters source,\n",
      "understand, engage and manage talent, using Generative AI / proprietary\n",
      "LLMs. Our patented model matches people with roles with greater accuracy\n",
      "and speed than previously imaginable ‚Äî no keywords required.\n",
      "Our long term goal is to help people discover their potential and pursue their\n",
      "reason for being, motivated by a concept called Ikigai. We help people find\n",
      "roles where they will be most fulfilled and successful; as a result, we will raise\n",
      "the level of human prosperity. It sounds grandiose, but since 77% of people\n",
      "don‚Äôt consider themselves inspired or engaged at work, it‚Äôs completely within\n",
      "our reach.\n",
      "Simplified.Travel\n",
      "AI Advisor\n",
      "February 2025¬†-¬†Present¬†(2 months)\n",
      "Simplified Travel is empowering destinations to deliver unforgettable, data-\n",
      "driven journeys at scale.\n",
      "I'm giving AI advice to enable highly personalized itinerary solutions for DMOs,\n",
      "hotels and tourism organizations, enhancing traveler experiences.\n",
      "GQR Global Markets\n",
      "Chief Technology Officer\n",
      "January 2020¬†-¬†Present¬†(5 years 3 months)\n",
      "New York, New York, United States\n",
      "As CTO of parent company Wynden Stark, I'm also responsible for innovation\n",
      "initiatives at GQR.\n",
      "Wynden Stark\n",
      "Chief Technology Officer\n",
      "January 2020¬†-¬†Present¬†(5 years 3 months)\n",
      "New York, New York, United States\n",
      "With the acquisition of untapt, I transitioned to Chief Technology Officer for the\n",
      "Wynden Stark Group, responsible for Data Science and Engineering.\n",
      "¬† Page 2 of 5¬† ¬†\n",
      "untapt\n",
      "6 years 4 months\n",
      "Founder, CTO\n",
      "May 2019¬†-¬†January 2020¬†(9 months)\n",
      "Greater New York City Area\n",
      "I founded untapt in October 2013; emerged from stealth in 2014 and went\n",
      "into production with first product in 2015. In May 2019, I handed over CEO\n",
      "responsibilities to Gareth Moody, previously the Chief Revenue Officer, shifting\n",
      "my focus to the technology and product.\n",
      "Our core invention is an Artificial Neural Network that uses Deep Learning /\n",
      "NLP to understand the fit between candidates and roles.\n",
      "Our SaaS products are used in the Recruitment Industry to connect people\n",
      "with jobs in a highly scalable way. Our products are also used by Corporations\n",
      "for internal and external hiring at high volume. We have strong SaaS metrics\n",
      "and trends, and a growing number of bellwether clients.\n",
      "Our Deep Learning / NLP models are developed in Python using Google\n",
      "TensorFlow. Our tech stack is React / Redux and Angular HTML5 front-end\n",
      "with Python / Flask back-end and MongoDB database. We are deployed on\n",
      "the Google Cloud Platform using Kubernetes container orchestration.\n",
      "Interview at NASDAQ: https://www.pscp.tv/w/1mnxeoNrEvZGX\n",
      "Founder, CEO\n",
      "October 2013¬†-¬†May 2019¬†(5 years 8 months)\n",
      "Greater New York City Area\n",
      "I founded untapt in October 2013; emerged from stealth in 2014 and went into\n",
      "production with first product in 2015.\n",
      "Our core invention is an Artificial Neural Network that uses Deep Learning /\n",
      "NLP to understand the fit between candidates and roles.\n",
      "Our SaaS products are used in the Recruitment Industry to connect people\n",
      "with jobs in a highly scalable way. Our products are also used by Corporations\n",
      "for internal and external hiring at high volume. We have strong SaaS metrics\n",
      "and trends, and a growing number of bellwether clients.\n",
      "¬† Page 3 of 5¬† ¬†\n",
      "Our Deep Learning / NLP models are developed in Python using Google\n",
      "TensorFlow. Our tech stack is React / Redux and Angular HTML5 front-end\n",
      "with Python / Flask back-end and MongoDB database. We are deployed on\n",
      "the Google Cloud Platform using Kubernetes container orchestration.\n",
      "-- Graduate of FinTech Innovation Lab\n",
      "-- American Banker Top 20 Company To Watch\n",
      "-- Voted AWS startup most likely to grow exponentially\n",
      "-- Forbes contributor\n",
      "More at https://www.untapt.com\n",
      "Interview at NASDAQ: https://www.pscp.tv/w/1mnxeoNrEvZGX\n",
      "In Fast Company: https://www.fastcompany.com/3067339/how-artificial-\n",
      "intelligence-is-changing-the-way-companies-hire\n",
      "JPMorgan Chase\n",
      "11 years 6 months\n",
      "Managing Director\n",
      "May 2011¬†-¬†March 2013¬†(1 year 11 months)\n",
      "Head of Technology for the Credit Portfolio Group and Hedge Fund Credit in\n",
      "the JPMorgan Investment Bank.\n",
      "Led a team of 300 Java and Python software developers across NY, Houston,\n",
      "London, Glasgow and India. Responsible for counterparty exposure, CVA\n",
      "and risk management platforms, including simulation engines in Python that\n",
      "calculate counterparty credit risk for the firm's Derivatives portfolio.\n",
      "Managed the electronic trading limits initiative, and the Credit Stress program\n",
      "which calculates risk information under stressed conditions. Jointly responsible\n",
      "for Market Data and batch infrastructure across Risk.\n",
      "Executive Director\n",
      "January 2007¬†-¬†May 2011¬†(4 years 5 months)\n",
      "From Jan 2008:\n",
      "Chief Business Technologist for the Credit Portfolio Group and Hedge Fund\n",
      "Credit in the JPMorgan Investment Bank, building Java and Python solutions\n",
      "and managing a team of full stack developers.\n",
      "2007:\n",
      "¬† Page 4 of 5¬† ¬†\n",
      "Responsible for Credit Risk Limits Monitoring infrastructure for Derivatives and\n",
      "Cash Securities, developed in Java / Javascript / HTML.\n",
      "VP\n",
      "July 2004¬†-¬†December 2006¬†(2 years 6 months)\n",
      "Managed Collateral, Netting and Legal documentation technology across\n",
      "Derivatives, Securities and Traditional Credit Products, including Java, Oracle,\n",
      "SQL based platforms\n",
      "VP\n",
      "October 2001¬†-¬†June 2004¬†(2 years 9 months)\n",
      "Full stack developer, then manager for Java cross-product risk management\n",
      "system in Credit Markets Technology\n",
      "Cygnifi\n",
      "Project Leader\n",
      "January 2000¬†-¬†September 2001¬†(1 year 9 months)\n",
      "Full stack developer and engineering lead, developing Java and Javascript\n",
      "platform to risk manage Interest Rate Derivatives at this FInTech startup and\n",
      "JPMorgan spin-off.\n",
      "JPMorgan\n",
      "Associate\n",
      "July 1997¬†-¬†December 1999¬†(2 years 6 months)\n",
      "Full stack developer for Exotic and Flow Interest Rate Derivatives risk\n",
      "management system in London, New York and Tokyo\n",
      "IBM\n",
      "Software Developer\n",
      "August 1995¬†-¬†June 1997¬†(1 year 11 months)\n",
      "Java and Smalltalk developer with IBM Global Services; taught IBM classes on\n",
      "Smalltalk and Object Technology in the UK and around Europe\n",
      "Education\n",
      "University of Oxford\n",
      "Physics¬†¬†¬∑¬†(1992¬†-¬†1995)\n",
      "¬† Page 5 of 5\n"
     ]
    }
   ],
   "source": [
    "# Add LinkedIn profile information (optional)\n",
    "linkedin = \"\"\"\n",
    "LinkedIn: linkedin.com/in/johnsmith-dev\n",
    "\n",
    "Recent posts and activities:\n",
    "- Shared insights about AI in software development\n",
    "- Posted about recent project using OpenAI API\n",
    "- Engaged with posts about Python best practices\n",
    "- Connected with other developers and AI enthusiasts\n",
    "\n",
    "Recommendations received:\n",
    "\"John is an exceptional developer with strong AI/ML skills. His work on our chatbot project was outstanding.\" - Former Manager\n",
    "\n",
    "\"Collaborative team player who consistently delivers high-quality code.\" - Colleague\n",
    "\"\"\"\n",
    "\n",
    "# Combine all content\n",
    "summary = text + \"\\n\\n\" + linkedin\n",
    "\n",
    "print(\"[INFO] Content preparation complete\")\n",
    "print(f\"Total knowledge base: {len(summary)} characters\")\n",
    "print(\"\\nContent includes:\")\n",
    "print(\"- Professional experience and skills\")  \n",
    "print(\"- Education and certifications\")\n",
    "print(\"- Project portfolio\")\n",
    "print(\"- LinkedIn activity and recommendations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Personal summary loaded (387 characters)\n",
      "üìù Update me/summary.txt with your own background!\n"
     ]
    }
   ],
   "source": [
    "# Setup AI clients based on available API keys\n",
    "clients = {}\n",
    "\n",
    "# OpenAI setup\n",
    "if os.getenv('OPENAI_API_KEY'):\n",
    "    openai = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "    clients['OpenAI'] = openai\n",
    "    print(\"[SETUP] OpenAI client configured\")\n",
    "\n",
    "# Google Gemini setup (using OpenAI-compatible interface)\n",
    "if os.getenv('GOOGLE_API_KEY'):\n",
    "    gemini = OpenAI(\n",
    "        api_key=os.getenv('GOOGLE_API_KEY'),\n",
    "        base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "    )\n",
    "    clients['Gemini'] = gemini\n",
    "    print(\"[SETUP] Gemini client configured\")\n",
    "\n",
    "# Qwen setup\n",
    "if os.getenv('DASHSCOPE_API_KEY'):\n",
    "    qwen_chat = OpenAI(\n",
    "        api_key=os.getenv('DASHSCOPE_API_KEY'),\n",
    "        base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\"\n",
    "    )\n",
    "    clients['Qwen'] = qwen_chat\n",
    "    print(\"[SETUP] Qwen client configured\")\n",
    "\n",
    "print(f\"\\nTotal clients available: {len(clients)}\")\n",
    "if not clients:\n",
    "    print(\"[WARNING] No API keys found - please configure at least one API key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Chatbot will represent: Ed Donner\n",
      "‚ö†Ô∏è  Remember to change this to YOUR name!\n"
     ]
    }
   ],
   "source": [
    "# Test the chatbot with different clients\n",
    "system_prompt = f\"\"\"You are a helpful assistant that answers questions about this person's professional background and experience. Here is their information:\n",
    "\n",
    "{summary}\n",
    "\n",
    "Guidelines:\n",
    "- Answer questions based only on the provided information\n",
    "- Be friendly and professional\n",
    "- If asked about something not in the provided info, say \"I don't have that specific information\"\n",
    "- Keep responses concise but informative\n",
    "\"\"\"\n",
    "\n",
    "def test_chatbot(question, client_name):\n",
    "    \"\"\"Test the chatbot with a specific client\"\"\"\n",
    "    if client_name not in clients:\n",
    "        return f\"Client {client_name} not available\"\n",
    "    \n",
    "    client = clients[client_name]\n",
    "    \n",
    "    try:\n",
    "        # Select appropriate model based on client\n",
    "        if client_name == 'OpenAI':\n",
    "            model = \"gpt-4o-mini\"\n",
    "        elif client_name == 'Gemini':\n",
    "            model = \"gemini-1.5-flash\"\n",
    "        elif client_name == 'Qwen':\n",
    "            model = \"qwen2.5-72b-instruct\"\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": question}\n",
    "            ],\n",
    "            max_tokens=200\n",
    "        )\n",
    "        \n",
    "        return response.choices[0].message.content\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error with {client_name}: {str(e)}\"\n",
    "\n",
    "# Test with first available client\n",
    "test_question = \"What programming languages does this person know?\"\n",
    "\n",
    "for client_name in clients.keys():\n",
    "    print(f\"\\n[TESTING] {client_name} Response:\")\n",
    "    print(\"-\" * 40)\n",
    "    test_reply = test_chatbot(test_question, client_name)\n",
    "    print(test_reply)\n",
    "    break  # Test with just the first available client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ System prompt created\n",
      "üìè Prompt length: 9305 characters\n",
      "üé≠ Your chatbot now has personality and context!\n"
     ]
    }
   ],
   "source": [
    "# Quality control evaluation system\n",
    "class Evaluation(BaseModel):\n",
    "    accuracy: int  # 1-10 scale\n",
    "    helpfulness: int  # 1-10 scale  \n",
    "    completeness: int  # 1-10 scale\n",
    "    overall_score: int  # 1-10 scale\n",
    "    feedback: str\n",
    "\n",
    "evaluator_system_prompt = \"\"\"You are an expert evaluator assessing chatbot responses about professional profiles. \n",
    "\n",
    "Rate each response on:\n",
    "- Accuracy (1-10): How factually correct is the response?\n",
    "- Helpfulness (1-10): How useful is this response to the user?\n",
    "- Completeness (1-10): How complete is the answer?\n",
    "- Overall Score (1-10): Your overall assessment\n",
    "\n",
    "Provide constructive feedback explaining your scores.\"\"\"\n",
    "\n",
    "def evaluate_response(question, response, evaluator_client):\n",
    "    \"\"\"Evaluate a chatbot response for quality\"\"\"\n",
    "    \n",
    "    evaluation_prompt = f\"\"\"\n",
    "Question: {question}\n",
    "Response to evaluate: {response}\n",
    "\n",
    "Please evaluate this chatbot response using the criteria above. Return your evaluation in the specified JSON format.\n",
    "\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Use OpenAI for evaluation if available, otherwise Qwen\n",
    "        if evaluator_client == 'OpenAI' and 'OpenAI' in clients:\n",
    "            eval_response = clients['OpenAI'].chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": evaluator_system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": evaluation_prompt}\n",
    "                ],\n",
    "                max_tokens=300\n",
    "            )\n",
    "        elif evaluator_client == 'Qwen' and 'Qwen' in clients:\n",
    "            eval_response = clients['Qwen'].chat.completions.create(\n",
    "                model=\"qwen2.5-72b-instruct\", \n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": evaluator_system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": evaluation_prompt}\n",
    "                ],\n",
    "                max_tokens=300\n",
    "            )\n",
    "        else:\n",
    "            return \"No evaluator client available\"\n",
    "            \n",
    "        eval_text = eval_response.choices[0].message.content\n",
    "        \n",
    "        # Try to extract structured evaluation\n",
    "        if \"accuracy\" in eval_text.lower():\n",
    "            return eval_text\n",
    "        else:\n",
    "            return f\"Evaluation completed: {eval_text}\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        return f\"Evaluation error: {str(e)}\"\n",
    "\n",
    "# Test the evaluation system\n",
    "print(\"[TESTING] Quality Control System\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "if test_reply:\n",
    "    # Choose evaluator (prefer OpenAI, fallback to Qwen)\n",
    "    evaluator = 'OpenAI' if 'OpenAI' in clients else 'Qwen' if 'Qwen' in clients else None\n",
    "    \n",
    "    if evaluator:\n",
    "        print(f\"Using {evaluator} as evaluator\")\n",
    "        evaluation = evaluate_response(test_question, test_reply, evaluator)\n",
    "        print(\"\\nEVALUATION RESULTS:\")\n",
    "        print(evaluation)\n",
    "    else:\n",
    "        print(\"No evaluator available\")\n",
    "else:\n",
    "    print(\"No response to evaluate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Option 1 - Multi-Model Approach (OpenAI + Gemini)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section demonstrates using multiple AI models with quality control. It compares responses from different models and provides evaluation feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhanced_chatbot(question):\n",
    "    \"\"\"Enhanced chatbot that uses multiple models and evaluation\"\"\"\n",
    "    \n",
    "    print(f\"QUESTION: {question}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    responses = {}\n",
    "    evaluations = {}\n",
    "    \n",
    "    # Get responses from available models\n",
    "    for client_name in ['OpenAI', 'Gemini']:\n",
    "        if client_name in clients:\n",
    "            print(f\"\\n[{client_name}] Generating response...\")\n",
    "            response = test_chatbot(question, client_name)\n",
    "            responses[client_name] = response\n",
    "            \n",
    "            # Evaluate the response\n",
    "            evaluator = 'OpenAI' if 'OpenAI' in clients else 'Qwen'\n",
    "            if evaluator in clients:\n",
    "                evaluation = evaluate_response(question, response, evaluator)\n",
    "                evaluations[client_name] = evaluation\n",
    "    \n",
    "    # Display results\n",
    "    for client_name, response in responses.items():\n",
    "        print(f\"\\n[RESPONSE - {client_name}]\")\n",
    "        print(\"-\" * 30)\n",
    "        print(response)\n",
    "        \n",
    "        if client_name in evaluations:\n",
    "            print(f\"\\n[EVALUATION - {client_name}]\")\n",
    "            print(evaluations[client_name])\n",
    "            print(\"-\" * 30)\n",
    "    \n",
    "    return responses, evaluations\n",
    "\n",
    "# Test the enhanced system\n",
    "if len(clients) >= 1:\n",
    "    print(\"Testing enhanced multi-model chatbot...\")\n",
    "    \n",
    "    test_questions = [\n",
    "        \"What is this person's main area of expertise?\",\n",
    "        \"What kind of projects has this person worked on?\", \n",
    "        \"What are their technical skills?\"\n",
    "    ]\n",
    "    \n",
    "    for question in test_questions[:1]:  # Test with first question\n",
    "        results = enhanced_chatbot(question)\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        \n",
    "else:\n",
    "    print(\"Need at least one API client to run enhanced chatbot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìö Section 3: Test Basic Chat (OpenAI Path)\n",
    "\n",
    "Let's test the basic chatbot before adding quality control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the basic OpenAI chatbot (without quality control)\n",
    "print(\"üöÄ Launching basic OpenAI chatbot for testing...\")\n",
    "print(\"üß™ This is just a test - we'll add quality control next\")\n",
    "\n",
    "gr.ChatInterface(chat_openai, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: Option 2 - Complete Qwen Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Qwen clients ready!\n",
      "üí¨ Chat model: qwen-plus\n",
      "üîç Evaluator model: qwen-max\n"
     ]
    }
   ],
   "source": [
    "# Complete Qwen-based solution\n",
    "if os.getenv('DASHSCOPE_API_KEY'):\n",
    "    print(\"[QWEN SETUP] Configuring complete Qwen solution...\")\n",
    "    \n",
    "    # Main chatbot client  \n",
    "    qwen_chat = OpenAI(\n",
    "        api_key=os.getenv('DASHSCOPE_API_KEY'),\n",
    "        base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\"\n",
    "    )\n",
    "    \n",
    "    # Separate evaluator client (using same API but different system prompt)\n",
    "    qwen_evaluator = OpenAI(\n",
    "        api_key=os.getenv('DASHSCOPE_API_KEY'),\n",
    "        base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\"\n",
    "    )\n",
    "    \n",
    "    print(\"[QWEN SETUP] Chat client configured\")\n",
    "    print(\"[QWEN SETUP] Evaluator client configured\")\n",
    "    print(\"[QWEN SETUP] Ready for complete Qwen workflow\")\n",
    "    \n",
    "else:\n",
    "    print(\"[ERROR] DASHSCOPE_API_KEY not found!\")\n",
    "    print(\"Please configure your Qwen API key to use this section.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Checking Qwen Version Dependencies ===\n",
      "‚úÖ name: Available\n",
      "‚úÖ summary: Available\n",
      "‚úÖ linkedin: Available\n",
      "‚úÖ system_prompt: Available\n",
      "‚úÖ DASHSCOPE_API_KEY: Available (sk-0958b...)\n",
      "\n",
      "üéâ ALL DEPENDENCIES READY! Qwen version can proceed.\n"
     ]
    }
   ],
   "source": [
    "def qwen_chatbot(question):\n",
    "    \"\"\"Pure Qwen chatbot implementation\"\"\"\n",
    "    \n",
    "    if not os.getenv('DASHSCOPE_API_KEY'):\n",
    "        return \"Qwen API key not configured\"\n",
    "    \n",
    "    try:\n",
    "        response = qwen_chat.chat.completions.create(\n",
    "            model=\"qwen2.5-72b-instruct\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": question}\n",
    "            ],\n",
    "            max_tokens=250\n",
    "        )\n",
    "        \n",
    "        return response.choices[0].message.content\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Qwen error: {str(e)}\"\n",
    "\n",
    "def qwen_evaluate(question, response):\n",
    "    \"\"\"Qwen-based evaluation\"\"\"\n",
    "    \n",
    "    if not os.getenv('DASHSCOPE_API_KEY'):\n",
    "        return \"Qwen evaluator not available\"\n",
    "    \n",
    "    evaluation_prompt = f\"\"\"\n",
    "Question: {question}\n",
    "Response to evaluate: {response}\n",
    "\n",
    "Please evaluate this response on a scale of 1-10 for:\n",
    "- Accuracy: How factually correct is it?\n",
    "- Helpfulness: How useful is it to the user? \n",
    "- Completeness: How complete is the answer?\n",
    "\n",
    "Provide scores and brief feedback.\n",
    "\"\"\"\n",
    "    \n",
    "    try:\n",
    "        eval_response = qwen_evaluator.chat.completions.create(\n",
    "            model=\"qwen2.5-72b-instruct\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": evaluator_system_prompt},\n",
    "                {\"role\": \"user\", \"content\": evaluation_prompt}\n",
    "            ],\n",
    "            max_tokens=200\n",
    "        )\n",
    "        \n",
    "        return eval_response.choices[0].message.content\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Evaluation error: {str(e)}\"\n",
    "\n",
    "# Test Qwen system\n",
    "if os.getenv('DASHSCOPE_API_KEY'):\n",
    "    print(\"Testing complete Qwen solution...\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    qwen_question = \"What are this person's key technical skills?\"\n",
    "    \n",
    "    print(f\"Question: {qwen_question}\")\n",
    "    print(\"\\n[QWEN RESPONSE]\")\n",
    "    qwen_reply = qwen_chatbot(qwen_question)\n",
    "    print(qwen_reply)\n",
    "    \n",
    "    print(\"\\n[QWEN EVALUATION]\") \n",
    "    qwen_eval = qwen_evaluate(qwen_question, qwen_reply)\n",
    "    print(qwen_eval)\n",
    "    \n",
    "else:\n",
    "    print(\"Qwen API key required for this section\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LinkedIn PDF loaded successfully\n",
      "‚úÖ Summary loaded successfully\n",
      "‚úÖ Name set to: Ed Donner\n",
      "‚úÖ System prompt created\n",
      "\n",
      "üéâ All Qwen dependencies now loaded! You can proceed with the Qwen cells below.\n"
     ]
    }
   ],
   "source": [
    "def qwen_with_context_awareness(question):\n",
    "    \"\"\"Enhanced Qwen chatbot with better context awareness\"\"\"\n",
    "    \n",
    "    enhanced_prompt = f\"\"\"You are an expert assistant answering questions about a professional's background. Here's their information:\n",
    "\n",
    "{summary}\n",
    "\n",
    "Key instructions:\n",
    "- Base your answers strictly on the provided information\n",
    "- If information is missing, clearly state what you don't know\n",
    "- Provide specific examples when available\n",
    "- Be conversational but professional\n",
    "- Focus on the most relevant details for each question\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "    if not os.getenv('DASHSCOPE_API_KEY'):\n",
    "        return \"Qwen API not configured\"\n",
    "    \n",
    "    try:\n",
    "        response = qwen_chat.chat.completions.create(\n",
    "            model=\"qwen2.5-72b-instruct\",\n",
    "            messages=[{\"role\": \"user\", \"content\": enhanced_prompt}],\n",
    "            max_tokens=300\n",
    "        )\n",
    "        \n",
    "        return response.choices[0].message.content\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "def qwen_batch_qa(questions):\n",
    "    \"\"\"Process multiple questions efficiently\"\"\"\n",
    "    \n",
    "    print(\"[BATCH PROCESSING] Processing multiple questions...\")\n",
    "    results = {}\n",
    "    \n",
    "    for i, question in enumerate(questions, 1):\n",
    "        print(f\"\\n[Question {i}/{len(questions)}] {question}\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        response = qwen_with_context_awareness(question)\n",
    "        results[question] = response\n",
    "        \n",
    "        print(response)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test enhanced features\n",
    "if os.getenv('DASHSCOPE_API_KEY'):\n",
    "    test_questions = [\n",
    "        \"What is this person's educational background?\",\n",
    "        \"What projects have they worked on?\",\n",
    "        \"What makes them a good candidate for a software role?\"\n",
    "    ]\n",
    "    \n",
    "    print(\"Testing enhanced Qwen features...\")\n",
    "    batch_results = qwen_batch_qa(test_questions)\n",
    "    \n",
    "else:\n",
    "    print(\"Qwen API key needed for enhanced features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testing Qwen API Connection ===\n",
      "‚úÖ Qwen API connection successful!\n",
      "Response: Hello, Qwen API works!\n",
      "‚úÖ Qwen API connection successful!\n",
      "Response: Hello, Qwen API works!\n"
     ]
    }
   ],
   "source": [
    "def qwen_quality_system(question, quality_threshold=7):\n",
    "    \"\"\"Complete quality control system using only Qwen\"\"\"\n",
    "    \n",
    "    print(f\"[QUALITY SYSTEM] Processing: {question}\")\n",
    "    print(f\"Quality threshold: {quality_threshold}/10\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    # Get response\n",
    "    print(\"[STEP 1] Generating response...\")\n",
    "    response = qwen_with_context_awareness(question)\n",
    "    \n",
    "    # Evaluate response\n",
    "    print(\"[STEP 2] Evaluating quality...\")\n",
    "    evaluation = qwen_evaluate(question, response)\n",
    "    \n",
    "    # Simple quality check (in practice, you'd parse the evaluation)\n",
    "    quality_indicators = [\n",
    "        len(response) > 100,  # Substantial response\n",
    "        \"don't know\" not in response.lower() or \"information\" in response.lower(),  # Informative\n",
    "        any(keyword in response.lower() for keyword in ['experience', 'skills', 'project', 'work'])  # Relevant\n",
    "    ]\n",
    "    \n",
    "    quality_score = sum(quality_indicators) * 3  # Simple scoring\n",
    "    \n",
    "    print(f\"[STEP 3] Quality score: {quality_score}/10\")\n",
    "    \n",
    "    if quality_score >= quality_threshold:\n",
    "        print(\"[SUCCESS] Quality threshold met!\")\n",
    "        status = \"APPROVED\"\n",
    "    else:\n",
    "        print(\"[WARNING] Below quality threshold\")\n",
    "        status = \"NEEDS_IMPROVEMENT\"\n",
    "    \n",
    "    return {\n",
    "        'question': question,\n",
    "        'response': response,\n",
    "        'evaluation': evaluation,\n",
    "        'quality_score': quality_score,\n",
    "        'status': status\n",
    "    }\n",
    "\n",
    "# Test complete quality system\n",
    "if os.getenv('DASHSCOPE_API_KEY'):\n",
    "    print(\"Testing complete Qwen quality system...\")\n",
    "    \n",
    "    quality_test = qwen_quality_system(\n",
    "        \"What programming experience does this person have?\",\n",
    "        quality_threshold=6\n",
    "    )\n",
    "    \n",
    "    print(\"\\n[FINAL RESULTS]\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"Status: {quality_test['status']}\")\n",
    "    print(f\"Quality Score: {quality_test['quality_score']}\")\n",
    "    print(f\"\\nResponse:\\n{quality_test['response']}\")\n",
    "    \n",
    "else:\n",
    "    print(\"Complete Qwen system requires DASHSCOPE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_all_approaches(question):\n",
    "    \"\"\"Compare responses from all available models\"\"\"\n",
    "    \n",
    "    print(f\"[COMPARISON] Question: {question}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Test each available model\n",
    "    available_models = {\n",
    "        'OpenAI': lambda q: test_chatbot(q, 'OpenAI') if 'OpenAI' in clients else None,\n",
    "        'Gemini': lambda q: test_chatbot(q, 'Gemini') if 'Gemini' in clients else None,\n",
    "        'Qwen': lambda q: qwen_chatbot(q) if os.getenv('DASHSCOPE_API_KEY') else None\n",
    "    }\n",
    "    \n",
    "    for model_name, model_func in available_models.items():\n",
    "        response = model_func(question)\n",
    "        if response:\n",
    "            results[model_name] = response\n",
    "            print(f\"\\n[{model_name}] Response:\")\n",
    "            print(\"-\" * 25)\n",
    "            print(response)\n",
    "    \n",
    "    print(f\"\\n[SUMMARY] Tested {len(results)} models\")\n",
    "    return results\n",
    "\n",
    "# Run comparison if any models are available\n",
    "if clients or os.getenv('DASHSCOPE_API_KEY'):\n",
    "    comparison_question = \"What is this person's main expertise?\"\n",
    "    comparison_results = compare_all_approaches(comparison_question)\n",
    "else:\n",
    "    print(\"No models available for comparison\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_conversation():\n",
    "    \"\"\"Simulate a conversation flow\"\"\"\n",
    "    \n",
    "    conversation = [\n",
    "        \"Hi! Can you tell me about this person's background?\",\n",
    "        \"What kind of projects have they worked on?\", \n",
    "        \"What are their main technical skills?\",\n",
    "        \"What makes them qualified for AI/ML roles?\"\n",
    "    ]\n",
    "    \n",
    "    print(\"[CONVERSATION SIMULATION]\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Choose best available model\n",
    "    if 'OpenAI' in clients:\n",
    "        model_func = lambda q: test_chatbot(q, 'OpenAI')\n",
    "        model_name = \"OpenAI\"\n",
    "    elif os.getenv('DASHSCOPE_API_KEY'):\n",
    "        model_func = qwen_chatbot\n",
    "        model_name = \"Qwen\"\n",
    "    else:\n",
    "        print(\"No models available for conversation\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Using {model_name} for conversation\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for i, question in enumerate(conversation, 1):\n",
    "        print(f\"\\n[Turn {i}] User: {question}\")\n",
    "        response = model_func(question)\n",
    "        print(f\"[Turn {i}] Assistant: {response}\")\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "# Run conversation simulation\n",
    "simulate_conversation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 4: Web Interface and Deployment\n",
    "    \n",
    "    print(f\"Analyzing {model_name} performance...\")\n",
    "    \n",
    "    for i, scenario in enumerate(test_scenarios, 1):\n",
    "        print(f\"\\n[Scenario {i}] {scenario}\")\n",
    "        print(\"-\" * 35)\n",
    "        \n",
    "        response = model_func(scenario)\n",
    "        \n",
    "        # Simple analysis metrics\n",
    "        response_length = len(response)\n",
    "        has_specific_info = any(word in response.lower() \n",
    "                              for word in ['python', 'javascript', 'experience', 'project'])\n",
    "        \n",
    "        print(f\"Response: {response}\")\n",
    "        print(f\"Length: {response_length} characters\")\n",
    "        print(f\"Contains specific info: {'Yes' if has_specific_info else 'No'}\")\n",
    "    \n",
    "    print(f\"\\n[ANALYSIS COMPLETE] Tested {len(test_scenarios)} scenarios\")\n",
    "\n",
    "analyze_performance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üåü Section 5: Complete Qwen Version (Alternative Path)\n",
    "\n",
    "**Choose this path if you want a fully self-contained solution with Qwen models!**\n",
    "\n",
    "## üéØ What This Path Offers:\n",
    "- **Qwen-Plus** for main chat functionality (fast, smart responses)\n",
    "- **Qwen-Max** for evaluation (premium model for quality control)\n",
    "- **No geographic restrictions** - works globally\n",
    "- **Self-contained** - only needs DASHSCOPE_API_KEY\n",
    "- **Same quality control** - automatic evaluation and retry\n",
    "\n",
    "## üìã Prerequisites:\n",
    "- `DASHSCOPE_API_KEY` in your `.env` file\n",
    "- All Section 1 cells completed (data loading)\n",
    "\n",
    "## üöÄ Quick Start:\n",
    "1. Run the dependency check below\n",
    "2. If needed, run the complete setup\n",
    "3. Test connectivity  \n",
    "4. Launch the Qwen chatbot!\n",
    "\n",
    "**Note:** You can run this section independently if you've completed Section 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Qwen clients configured!\n",
      "üí¨ Chat model: qwen-plus\n",
      "üîç Evaluator model: qwen-max\n",
      "üåç Global access - no geographic restrictions\n"
     ]
    }
   ],
   "source": [
    "def create_gradio_interface():\n",
    "    \"\"\"Create a web interface for the chatbot\"\"\"\n",
    "    \n",
    "    def chatbot_interface(question, model_choice):\n",
    "        \"\"\"Handle chatbot requests from web interface\"\"\"\n",
    "        \n",
    "        if not question.strip():\n",
    "            return \"Please enter a question!\"\n",
    "        \n",
    "        try:\n",
    "            if model_choice == \"OpenAI\" and 'OpenAI' in clients:\n",
    "                response = test_chatbot(question, 'OpenAI')\n",
    "            elif model_choice == \"Gemini\" and 'Gemini' in clients:\n",
    "                response = test_chatbot(question, 'Gemini')\n",
    "            elif model_choice == \"Qwen\" and os.getenv('DASHSCOPE_API_KEY'):\n",
    "                response = qwen_chatbot(question)\n",
    "            else:\n",
    "                response = f\"{model_choice} not available. Please configure the API key.\"\n",
    "            \n",
    "            return response\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"Error: {str(e)}\"\n",
    "    \n",
    "    # Create model options based on available APIs\n",
    "    model_options = []\n",
    "    if 'OpenAI' in clients:\n",
    "        model_options.append(\"OpenAI\")\n",
    "    if 'Gemini' in clients:\n",
    "        model_options.append(\"Gemini\")\n",
    "    if os.getenv('DASHSCOPE_API_KEY'):\n",
    "        model_options.append(\"Qwen\")\n",
    "    \n",
    "    if not model_options:\n",
    "        print(\"[ERROR] No models available for web interface\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"[GRADIO] Available models: {', '.join(model_options)}\")\n",
    "    \n",
    "    # Create the interface\n",
    "    interface = gr.Interface(\n",
    "        fn=chatbot_interface,\n",
    "        inputs=[\n",
    "            gr.Textbox(\n",
    "                label=\"Ask a question about the professional profile\",\n",
    "                placeholder=\"e.g., What are their technical skills?\",\n",
    "                lines=2\n",
    "            ),\n",
    "            gr.Dropdown(\n",
    "                choices=model_options,\n",
    "                label=\"Choose AI Model\",\n",
    "                value=model_options[0]\n",
    "            )\n",
    "        ],\n",
    "        outputs=gr.Textbox(label=\"Response\", lines=5),\n",
    "        title=\"Personal Website Chatbot\",\n",
    "        description=\"Ask questions about the professional background and experience.\",\n",
    "        examples=[\n",
    "            [\"What programming languages does this person know?\", model_options[0]],\n",
    "            [\"What kind of projects have they worked on?\", model_options[0]],\n",
    "            [\"What is their educational background?\", model_options[0]]\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return interface\n",
    "\n",
    "# Create and launch the interface\n",
    "print(\"Creating web interface...\")\n",
    "interface = create_gradio_interface()\n",
    "\n",
    "if interface:\n",
    "    print(\"[SUCCESS] Web interface created!\")\n",
    "    print(\"Run the next cell to launch the interface\")\n",
    "else:\n",
    "    print(\"[ERROR] Could not create interface - no models available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Qwen evaluation system ready\n",
      "üîç Qwen-Max will judge response quality\n",
      "üõ°Ô∏è  Robust error handling included\n"
     ]
    }
   ],
   "source": [
    "# Launch the web interface\n",
    "if 'interface' in locals() and interface is not None:\n",
    "    print(\"Launching chatbot web interface...\")\n",
    "    print(\"The interface will open in your browser\")\n",
    "    print(\"Use Ctrl+C to stop the server when done\")\n",
    "    \n",
    "    # Launch with share=False for local use only\n",
    "    # Set share=True if you want to create a public link\n",
    "    interface.launch(\n",
    "        share=False,  # Set to True for public sharing\n",
    "        server_name=\"127.0.0.1\",  # Local access only\n",
    "        server_port=7860  # Default Gradio port\n",
    "    )\n",
    "else:\n",
    "    print(\"Interface not available - please run the previous cell first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Complete Qwen workflow ready!\n",
      "ü§ñ Qwen-Plus chat + üîç Qwen-Max evaluation + üîÑ Auto-retry\n"
     ]
    }
   ],
   "source": [
    "def create_enhanced_interface():\n",
    "    \"\"\"Create an enhanced interface with quality control\"\"\"\n",
    "    \n",
    "    def enhanced_chatbot_interface(question, model_choice, include_evaluation):\n",
    "        \"\"\"Enhanced chatbot with optional evaluation\"\"\"\n",
    "        \n",
    "        if not question.strip():\n",
    "            return \"Please enter a question!\", \"\"\n",
    "        \n",
    "        try:\n",
    "            # Get response\n",
    "            if model_choice == \"OpenAI\" and 'OpenAI' in clients:\n",
    "                response = test_chatbot(question, 'OpenAI')\n",
    "            elif model_choice == \"Gemini\" and 'Gemini' in clients:\n",
    "                response = test_chatbot(question, 'Gemini') \n",
    "            elif model_choice == \"Qwen\" and os.getenv('DASHSCOPE_API_KEY'):\n",
    "                response = qwen_chatbot(question)\n",
    "            else:\n",
    "                return f\"{model_choice} not available\", \"\"\n",
    "            \n",
    "            # Get evaluation if requested\n",
    "            evaluation = \"\"\n",
    "            if include_evaluation:\n",
    "                if 'OpenAI' in clients:\n",
    "                    evaluation = evaluate_response(question, response, 'OpenAI')\n",
    "                elif os.getenv('DASHSCOPE_API_KEY'):\n",
    "                    evaluation = qwen_evaluate(question, response)\n",
    "                else:\n",
    "                    evaluation = \"Evaluation not available\"\n",
    "            \n",
    "            return response, evaluation\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"Error: {str(e)}\", \"\"\n",
    "    \n",
    "    # Get available models\n",
    "    model_options = []\n",
    "    if 'OpenAI' in clients:\n",
    "        model_options.append(\"OpenAI\")\n",
    "    if 'Gemini' in clients:\n",
    "        model_options.append(\"Gemini\")\n",
    "    if os.getenv('DASHSCOPE_API_KEY'):\n",
    "        model_options.append(\"Qwen\")\n",
    "    \n",
    "    if not model_options:\n",
    "        print(\"[ERROR] No models available\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"[ENHANCED] Creating interface with models: {', '.join(model_options)}\")\n",
    "    \n",
    "    # Create enhanced interface\n",
    "    enhanced_interface = gr.Interface(\n",
    "        fn=enhanced_chatbot_interface,\n",
    "        inputs=[\n",
    "            gr.Textbox(\n",
    "                label=\"Your Question\",\n",
    "                placeholder=\"Ask about technical skills, experience, projects...\",\n",
    "                lines=3\n",
    "            ),\n",
    "            gr.Dropdown(\n",
    "                choices=model_options,\n",
    "                label=\"AI Model\",\n",
    "                value=model_options[0]\n",
    "            ),\n",
    "            gr.Checkbox(\n",
    "                label=\"Include Quality Evaluation\",\n",
    "                value=False\n",
    "            )\n",
    "        ],\n",
    "        outputs=[\n",
    "            gr.Textbox(label=\"Chatbot Response\", lines=6),\n",
    "            gr.Textbox(label=\"Quality Evaluation\", lines=4)\n",
    "        ],\n",
    "        title=\"Enhanced Personal Chatbot\",\n",
    "        description=\"Professional background chatbot with optional quality evaluation\",\n",
    "        examples=[\n",
    "            [\"What technical skills does this person have?\", model_options[0], True],\n",
    "            [\"Tell me about their project experience\", model_options[0], False],\n",
    "            [\"What makes them qualified for AI roles?\", model_options[0], True]\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return enhanced_interface\n",
    "\n",
    "# Create enhanced interface\n",
    "print(\"Creating enhanced interface with quality control...\")\n",
    "enhanced_interface = create_enhanced_interface()\n",
    "\n",
    "if enhanced_interface:\n",
    "    print(\"[SUCCESS] Enhanced interface ready!\")\n",
    "    print(\"This interface includes quality evaluation features\")\n",
    "else:\n",
    "    print(\"[ERROR] Could not create enhanced interface\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing Qwen-Plus + Qwen-Max quality control...\n",
      "‚úÖ Qwen API connection confirmed\n",
      "üîç Evaluation test - Acceptable: False\n",
      "üí¨ Feedback: The response is inaccurate and misses an opportunity to highlight relevant information. Ed Donner's LinkedIn profile explicitly mentions a patented invention related to AI and talent, developed during his time at untapt. The Agent should have acknowledged this patent and provided details about it, since it is directly relevant to the question. Additionally, the response comes across as evasive rather than engaging or helpful, which does not align with the professional and informative tone expected when representing Ed Donner.\n"
     ]
    }
   ],
   "source": [
    "# Deployment considerations and tips\n",
    "print(\"DEPLOYMENT CONSIDERATIONS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "deployment_tips = [\n",
    "    \"Security: Never expose API keys in client-side code\",\n",
    "    \"Rate Limiting: Implement request limits to control costs\",\n",
    "    \"Error Handling: Always handle API failures gracefully\", \n",
    "    \"Monitoring: Track usage and response quality over time\",\n",
    "    \"Content Updates: Plan for updating the knowledge base\",\n",
    "    \"User Experience: Provide clear instructions and examples\"\n",
    "]\n",
    "\n",
    "for i, tip in enumerate(deployment_tips, 1):\n",
    "    print(f\"{i}. {tip}\")\n",
    "\n",
    "print(\"\\nDEPLOYMENT OPTIONS:\")\n",
    "print(\"-\" * 20)\n",
    "print(\"‚Ä¢ Local: Run on your computer (current setup)\")\n",
    "print(\"‚Ä¢ Cloud: Deploy to Heroku, Vercel, or similar\")\n",
    "print(\"‚Ä¢ Enterprise: Integrate into existing websites\")\n",
    "\n",
    "print(\"\\nNEXT STEPS:\")\n",
    "print(\"-\" * 15)\n",
    "print(\"‚Ä¢ Test with real questions from your domain\")\n",
    "print(\"‚Ä¢ Expand the knowledge base with more content\")\n",
    "print(\"‚Ä¢ Add authentication if deploying publicly\")\n",
    "print(\"‚Ä¢ Monitor and improve response quality\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Launching Qwen-powered personal website chatbot...\n",
      "üí¨ Main chat: Qwen-Plus (fast, efficient)\n",
      "üîç Quality control: Qwen-Max (premium evaluation)\n",
      "üîÑ Auto-retry on failed evaluation\n",
      "üåç Global access - no geographic restrictions\n",
      "\n",
      "Try asking about patents to see the pig latin feature!\n",
      "* Running on local URL:  http://127.0.0.1:7863\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Passed Qwen-Max evaluation - returning reply\n"
     ]
    }
   ],
   "source": [
    "# Launch the enhanced interface (optional)\n",
    "# Uncomment the lines below to launch the enhanced interface\n",
    "\n",
    "# if 'enhanced_interface' in locals() and enhanced_interface is not None:\n",
    "#     print(\"Launching enhanced interface...\")\n",
    "#     enhanced_interface.launch(share=False, server_port=7861)\n",
    "# else:\n",
    "#     print(\"Enhanced interface not available\")\n",
    "\n",
    "print(\"Enhanced interface ready to launch!\")\n",
    "print(\"Uncomment the code above to start the enhanced web interface\")\n",
    "print(\"Note: This will run on port 7861 to avoid conflicts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Excellent work! You've successfully built a sophisticated personal website chatbot with quality control features. Here's what you accomplished:\n",
    "\n",
    "**Technical Skills Demonstrated:**\n",
    "- PDF processing and content extraction\n",
    "- Multi-model AI integration (OpenAI, Gemini, Qwen)\n",
    "- Quality evaluation systems\n",
    "- Web interface development with Gradio\n",
    "- Error handling and retry logic\n",
    "\n",
    "**AI Agent Concepts Learned:**\n",
    "- Domain-specific knowledge integration\n",
    "- Response quality evaluation\n",
    "- Multi-model comparison and selection\n",
    "- Automated quality control systems\n",
    "- User interface design for AI applications\n",
    "\n",
    "**Real-World Applications:**\n",
    "- Personal portfolio chatbots\n",
    "- Customer service automation\n",
    "- Knowledge base querying systems\n",
    "- Educational content assistants\n",
    "- Professional screening tools\n",
    "\n",
    "**Key Takeaways:**\n",
    "- Quality control is essential for production AI systems\n",
    "- Different models have different strengths for different tasks\n",
    "- User experience design matters as much as AI performance\n",
    "- Proper error handling makes systems more robust\n",
    "- Content preparation significantly impacts chatbot quality\n",
    "\n",
    "**Next Steps:**\n",
    "- Expand to handle multiple document types (PDFs, websites, databases)\n",
    "- Add conversation memory for multi-turn interactions\n",
    "- Implement more sophisticated evaluation metrics\n",
    "- Add user feedback collection for continuous improvement\n",
    "- Explore integration with vector databases for larger knowledge bases\n",
    "\n",
    "You now have a solid foundation for building production-ready AI chatbot systems!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents (3.12.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
